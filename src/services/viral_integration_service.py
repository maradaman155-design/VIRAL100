# services/viral_integration_service.py
"""VIRAL IMAGE FINDER - ARQV30 Enhanced v3.0
M√≥dulo para buscar imagens virais no Google Imagens de Instagram/Facebook
Analisa engajamento, extrai links dos posts e salva dados estruturados
CORRIGIDO: APIs funcionais, extra√ß√£o real de imagens, fallbacks robustos
"""
import os
import re
import json
import time
import asyncio
import logging
import ssl
import random
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from urllib.parse import urlparse, parse_qs, unquote, urljoin
from dataclasses import dataclass, asdict
import hashlib

# Import condicional do Google Generative AI
try:
    import google.generativeai as genai
    HAS_GEMINI = True
except ImportError:
    HAS_GEMINI = False
    logger = logging.getLogger(__name__)
    logger.warning("google-generativeai n√£o encontrado.")

# Import condicional do Playwright
try:
    from playwright.async_api import async_playwright, Page, Browser, BrowserContext
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("Playwright n√£o encontrado. Instale com 'pip install playwright' para funcionalidades avan√ßadas.")

# Imports ass√≠ncronos
try:
    import aiohttp
    import aiofiles
    HAS_ASYNC_DEPS = True
except ImportError:
    import requests
    HAS_ASYNC_DEPS = False
    logger = logging.getLogger(__name__)
    logger.warning("aiohttp/aiofiles n√£o encontrados. Usando requests s√≠ncrono como fallback.")

# BeautifulSoup para parsing HTML
try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False
    logger = logging.getLogger(__name__)
    logger.warning("BeautifulSoup4 n√£o encontrado.")

# Carregar vari√°veis de ambiente
from dotenv import load_dotenv
load_dotenv()

# Configura√ß√£o de logging
logger = logging.getLogger(__name__)

@dataclass
class ViralImage:
    """Estrutura de dados para imagem viral"""
    image_url: str
    post_url: str
    platform: str
    title: str
    description: str
    engagement_score: float
    views_estimate: int
    likes_estimate: int
    comments_estimate: int
    shares_estimate: int
    author: str
    author_followers: int
    post_date: str
    hashtags: List[str]
    image_path: Optional[str] = None
    screenshot_path: Optional[str] = None
    extracted_at: str = datetime.now().isoformat()

class ViralImageFinder:
    """Classe principal para encontrar imagens virais"""
    def __init__(self, config: Dict = None):
        self.config = config or self._load_config()
        # Sistema de rota√ß√£o de APIs
        self.api_keys = self._load_multiple_api_keys()
        self.current_api_index = {
            'apify': 0,
            'openrouter': 0,
            'serper': 0,
            'rapidapi': 0,
            'google_cse': 0
        }
        self.failed_apis = set()  # APIs que falharam recentemente
        self.instagram_session_cookie = self.config.get('instagram_session_cookie')
        self.playwright_enabled = self.config.get('playwright_enabled', True) and PLAYWRIGHT_AVAILABLE
        # Configurar diret√≥rios necess√°rios
        self._ensure_directories()
        # Configurar sess√£o HTTP s√≠ncrona para fallbacks
        if not HAS_ASYNC_DEPS:
            import requests
            self.session = requests.Session()
            self.setup_session()

    def _load_config(self) -> Dict:
        """Carrega configura√ß√µes do ambiente"""
        return {
            'gemini_api_key': os.getenv('GEMINI_API_KEY'),
            'serper_api_key': os.getenv('SERPER_API_KEY'),
            'google_search_key': os.getenv('GOOGLE_SEARCH_KEY'),
            'google_cse_id': os.getenv('GOOGLE_CSE_ID'),
            'apify_api_key': os.getenv('APIFY_API_KEY'),
            'instagram_session_cookie': os.getenv('INSTAGRAM_SESSION_COOKIE'),
            'rapidapi_key': os.getenv('RAPIDAPI_KEY'),
            'max_images': int(os.getenv('MAX_IMAGES', 30)),
            'min_engagement': float(os.getenv('MIN_ENGAGEMENT', 0)),
            'timeout': int(os.getenv('TIMEOUT', 30)),
            'headless': os.getenv('PLAYWRIGHT_HEADLESS', 'True').lower() == 'true',
            'output_dir': os.getenv('OUTPUT_DIR', 'viral_images_data'),
            'images_dir': os.getenv('IMAGES_DIR', 'downloaded_images'),
            'extract_images': os.getenv('EXTRACT_IMAGES', 'True').lower() == 'true',
            'playwright_enabled': os.getenv('PLAYWRIGHT_ENABLED', 'True').lower() == 'true',
            'screenshots_dir': os.getenv('SCREENSHOTS_DIR', 'screenshots'),
            'playwright_timeout': int(os.getenv('PLAYWRIGHT_TIMEOUT', 45000)),
            'playwright_browser': os.getenv('PLAYWRIGHT_BROWSER', 'chromium'),
        }

    def _load_multiple_api_keys(self) -> Dict:
        """Carrega m√∫ltiplas chaves de API para rota√ß√£o"""
        api_keys = {
            'apify': [],
            'openrouter': [],
            'serper': [],
            'rapidapi': [],
            'google_cse': []
        }
        # Apify - m√∫ltiplas chaves
        for i in range(1, 4):  # At√© 3 chaves Apify
            key = os.getenv(f'APIFY_API_KEY_{i}') or (os.getenv('APIFY_API_KEY') if i == 1 else None)
            if key and key.strip():
                api_keys['apify'].append(key.strip())
                logger.info(f"‚úÖ Apify API {i} carregada")
        # OpenRouter - m√∫ltiplas chaves
        for i in range(1, 4):  # At√© 3 chaves OpenRouter
            key = os.getenv(f'OPENROUTER_API_KEY_{i}') or (os.getenv('OPENROUTER_API_KEY') if i == 1 else None)
            if key and key.strip():
                api_keys['openrouter'].append(key.strip())
                logger.info(f"‚úÖ OpenRouter API {i} carregada")
        # Serper - m√∫ltiplas chaves
        for i in range(1, 3):  # At√© 2 chaves Serper
            key = os.getenv(f'SERPER_API_KEY_{i}') or (os.getenv('SERPER_API_KEY') if i == 1 else None)
            if key and key.strip():
                api_keys['serper'].append(key.strip())
                logger.info(f"‚úÖ Serper API {i} carregada")
        # RapidAPI - m√∫ltiplas chaves
        for i in range(1, 3):  # At√© 2 chaves RapidAPI
            key = os.getenv(f'RAPIDAPI_KEY_{i}') or (os.getenv('RAPIDAPI_KEY') if i == 1 else None)
            if key and key.strip():
                api_keys['rapidapi'].append(key.strip())
                logger.info(f"‚úÖ RapidAPI {i} carregada")
        # Google CSE
        google_key = os.getenv('GOOGLE_SEARCH_KEY')
        google_cse = os.getenv('GOOGLE_CSE_ID')
        if google_key and google_cse:
            api_keys['google_cse'].append({'key': google_key, 'cse_id': google_cse})
            logger.info(f"‚úÖ Google CSE carregada")
        return api_keys

    def _get_next_api_key(self, service: str) -> Optional[str]:
        """Obt√©m pr√≥xima chave de API dispon√≠vel com rota√ß√£o autom√°tica"""
        if service not in self.api_keys or not self.api_keys[service]:
            return None
        keys = self.api_keys[service]
        if not keys:
            return None
        # Tentar todas as chaves dispon√≠veis
        for attempt in range(len(keys)):
            current_index = self.current_api_index[service]
            # Verificar se esta API n√£o falhou recentemente
            api_identifier = f"{service}_{current_index}"
            if api_identifier not in self.failed_apis:
                key = keys[current_index]
                logger.info(f"üîÑ Usando {service} API #{current_index + 1}")
                # Avan√ßar para pr√≥xima API na pr√≥xima chamada
                self.current_api_index[service] = (current_index + 1) % len(keys)
                return key
            # Se esta API falhou, tentar a pr√≥xima
            self.current_api_index[service] = (current_index + 1) % len(keys)
        logger.error(f"‚ùå Todas as APIs de {service} falharam recentemente")
        return None

    def _mark_api_failed(self, service: str, index: int):
        """Marca uma API como falhada temporariamente"""
        api_identifier = f"{service}_{index}"
        self.failed_apis.add(api_identifier)
        logger.warning(f"‚ö†Ô∏è API {service} #{index + 1} marcada como falhada")
        # Limpar falhas ap√≥s 5 minutos (300 segundos)
        import threading
        def clear_failure():
            time.sleep(300)  # 5 minutos
            if api_identifier in self.failed_apis:
                self.failed_apis.remove(api_identifier)
                logger.info(f"‚úÖ API {service} #{index + 1} reabilitada")
        threading.Thread(target=clear_failure, daemon=True).start()

    def _ensure_directories(self):
        """Garante que todos os diret√≥rios necess√°rios existam"""
        dirs_to_create = [
            self.config['output_dir'],
            self.config['images_dir'],
            self.config['screenshots_dir']
        ]
        for directory in dirs_to_create:
            try:
                os.makedirs(directory, exist_ok=True)
                logger.info(f"‚úÖ Diret√≥rio criado/verificado: {directory}")
            except Exception as e:
                logger.error(f"‚ùå Erro ao criar diret√≥rio {directory}: {e}")

    def setup_session(self):
        """Configura sess√£o HTTP com headers apropriados"""
        if hasattr(self, 'session'):
            self.session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            })

    async def search_images(self, query: str) -> List[Dict]:
        """Busca imagens usando m√∫ltiplos provedores com estrat√©gia aprimorada"""
        all_results = []
        # Queries mais espec√≠ficas e eficazes para conte√∫do educacional
        queries = [
            # Instagram queries - mais variadas
            f'"{query}" site:instagram.com',
            f'site:instagram.com/p "{query}"',
            f'site:instagram.com/reel "{query}"',
            f'"{query}" instagram curso',
            f'"{query}" instagram masterclass',
            f'"{query}" instagram dicas',
            f'"{query}" instagram tutorial',
            # Facebook queries - mais robustas
            f'"{query}" site:facebook.com',
            f'site:facebook.com/posts "{query}"',
            f'"{query}" facebook curso',
            f'"{query}" facebook aula',
            f'"{query}" facebook dicas',
            # YouTube queries - para thumbnails
            f'"{query}" site:youtube.com',
            f'site:youtube.com/watch "{query}"',
            f'"{query}" youtube tutorial',
            f'"{query}" youtube curso',
            # Queries gerais mais amplas
            f'"{query}" curso online',
            f'"{query}" aula gratuita',
            f'"{query}" tutorial gratis',
            f'"{query}" masterclass'
        ]
        for q in queries[:8]:  # Aumentar para mais resultados
            logger.info(f"üîç Buscando: {q}")
            results = []
            # Tentar Serper primeiro (mais confi√°vel)
            if self.config.get('serper_api_key'):
                try:
                    serper_results = await self._search_serper_advanced(q)
                    results.extend(serper_results)
                    logger.info(f"üìä Serper encontrou {len(serper_results)} resultados para: {q}")
                except Exception as e:
                    logger.error(f"‚ùå Erro na busca Serper para '{q}': {e}")
            # Google CSE como backup
            if len(results) < 3 and self.config.get('google_search_key') and self.config.get('google_cse_id'):
                try:
                    google_results = await self._search_google_cse_advanced(q)
                    results.extend(google_results)
                    logger.info(f"üìä Google CSE encontrou {len(google_results)} resultados para: {q}")
                except Exception as e:
                    logger.error(f"‚ùå Erro na busca Google CSE para '{q}': {e}")
            all_results.extend(results)
            # Rate limiting
            await asyncio.sleep(0.5)
        # RapidAPI Instagram como fonte adicional
        if self.config.get('rapidapi_key'):
            try:
                rapid_results = await self._search_rapidapi_instagram(query)
                all_results.extend(rapid_results)
                logger.info(f"üìä RapidAPI encontrou {len(rapid_results)} posts do Instagram")
            except Exception as e:
                logger.error(f"‚ùå Erro na busca RapidAPI: {e}")
        
        # YouTube thumbnails como fonte adicional
        try:
            youtube_results = await self._search_youtube_thumbnails(query)
            all_results.extend(youtube_results)
            logger.info(f"üì∫ YouTube thumbnails: {len(youtube_results)} encontrados")
        except Exception as e:
            logger.error(f"‚ùå Erro na busca YouTube: {e}")
        
        # Busca adicional espec√≠fica para Facebook
        try:
            facebook_results = await self._search_facebook_specific(query)
            all_results.extend(facebook_results)
            logger.info(f"üìò Facebook espec√≠fico: {len(facebook_results)} encontrados")
        except Exception as e:
            logger.error(f"‚ùå Erro na busca Facebook espec√≠fica: {e}")
        
        # Busca adicional com estrat√©gias alternativas se poucos resultados
        if len(all_results) < 15:
            try:
                alternative_results = await self._search_alternative_strategies(query)
                all_results.extend(alternative_results)
                logger.info(f"üîÑ Estrat√©gias alternativas: {len(alternative_results)} encontrados")
            except Exception as e:
                logger.error(f"‚ùå Erro nas estrat√©gias alternativas: {e}")
        
        # EXTRA√á√ÉO DIRETA DE POSTS ESPEC√çFICOS
        # Procurar por URLs espec√≠ficas nos resultados e extrair imagens diretamente
        direct_extraction_results = []
        instagram_urls = []
        facebook_urls = []
        linkedin_urls = []
        
        # Coletar URLs espec√≠ficas dos resultados
        for result in all_results:
            page_url = result.get('page_url', '')
            if 'instagram.com/p/' in page_url or 'instagram.com/reel/' in page_url:
                instagram_urls.append(page_url)
            elif 'facebook.com' in page_url:
                facebook_urls.append(page_url)
            elif 'linkedin.com' in page_url:
                linkedin_urls.append(page_url)
        
        # Extra√ß√£o direta do Instagram
        for insta_url in list(set(instagram_urls))[:5]:  # Limitar a 5 URLs
            try:
                direct_results = await self._extract_instagram_direct(insta_url)
                direct_extraction_results.extend(direct_results)
            except Exception as e:
                logger.warning(f"Erro extra√ß√£o direta Instagram {insta_url}: {e}")
        
        # Extra√ß√£o direta do Facebook
        for fb_url in list(set(facebook_urls))[:3]:  # Limitar a 3 URLs
            try:
                direct_results = await self._extract_facebook_direct(fb_url)
                direct_extraction_results.extend(direct_results)
            except Exception as e:
                logger.warning(f"Erro extra√ß√£o direta Facebook {fb_url}: {e}")
        
        # Extra√ß√£o direta do LinkedIn
        for li_url in list(set(linkedin_urls))[:3]:  # Limitar a 3 URLs
            try:
                direct_results = await self._extract_linkedin_direct(li_url)
                direct_extraction_results.extend(direct_results)
            except Exception as e:
                logger.warning(f"Erro extra√ß√£o direta LinkedIn {li_url}: {e}")
        
        # Adicionar resultados de extra√ß√£o direta
        all_results.extend(direct_extraction_results)
        logger.info(f"üéØ Extra√ß√£o direta: {len(direct_extraction_results)} imagens reais extra√≠das")
        # Remover duplicatas e filtrar URLs v√°lidos
        seen_urls = set()
        unique_results = []
        for result in all_results:
            post_url = result.get('page_url', '').strip()
            if post_url and post_url not in seen_urls and self._is_valid_social_url(post_url):
                seen_urls.add(post_url)
                unique_results.append(result)
        logger.info(f"üéØ Encontrados {len(unique_results)} posts √∫nicos e v√°lidos")
        return unique_results

    def _is_valid_social_url(self, url: str) -> bool:
        """Verifica se √© uma URL v√°lida de rede social (FILTRO RIGOROSO PARA MARKETING)"""
        if not url:
            return False
            
        # PRIMEIRO: Rejeitar URLs de CDN/imagens diretas (FILTRO ULTRA-RIGOROSO)
        invalid_patterns = [
            r'lookaside\.instagram\.com',
            r'scontent.*\.cdninstagram\.com',
            r'scontent.*\.fbcdn\.net',
            r'fbcdn\.net',
            r'media_id=',
            r'/seo/',
            r'graph\.instagram\.com',
            r'crawler/',
            r'widget/',
            r'google_widget',
            r'\.(jpg|jpeg|png|gif|webp|mp4)(\?|$)',  # Arquivos de m√≠dia
            r'cdninstagram',
            r'fbcdn',
            r'scontent-',
        ]
        
        for pattern in invalid_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return False
        
        # SEGUNDO: Aceitar apenas URLs diretas de posts
        valid_patterns = [
            r'https?://(?:www\.)?instagram\.com/p/[A-Za-z0-9_-]+/?$',      # Posts Instagram
            r'https?://(?:www\.)?instagram\.com/reel/[A-Za-z0-9_-]+/?$',   # Reels Instagram
            r'https?://(?:www\.)?instagram\.com/tv/[A-Za-z0-9_-]+/?$',     # IGTV
            r'https?://(?:www\.)?facebook\.com/[^/]+/posts/[0-9]+',        # Posts Facebook
            r'https?://(?:www\.)?facebook\.com/[^/]+/videos/[0-9]+',       # V√≠deos Facebook
            r'https?://(?:www\.)?facebook\.com/photo\.php\?fbid=[0-9]+',   # Fotos Facebook
            r'https?://(?:www\.)?facebook\.com/watch/?\?v=[0-9]+',         # Watch Facebook
            r'https?://(?:www\.)?youtube\.com/watch\?v=[A-Za-z0-9_-]+',    # YouTube
            r'https?://youtu\.be/[A-Za-z0-9_-]+',                          # YouTube curto
            r'https?://(?:www\.)?linkedin\.com/posts/[^/]+_[A-Za-z0-9_-]+', # LinkedIn posts
            r'https?://(?:www\.)?linkedin\.com/feed/update/[A-Za-z0-9:_-]+', # LinkedIn feed
            r'https?://(?:www\.)?tiktok\.com/@[^/]+/video/[0-9]+',         # TikTok
            r'https?://vm\.tiktok\.com/[A-Za-z0-9]+',                      # TikTok curto
        ]
        
        return any(re.search(pattern, url, re.IGNORECASE) for pattern in valid_patterns)

    def _is_valid_image_url(self, url: str) -> bool:
        """Verifica se a URL parece ser de uma imagem real"""
        if not url or not isinstance(url, str):
            return False
        
        # URLs que claramente n√£o s√£o imagens
        invalid_patterns = [
            r'instagram\.com/accounts/login',
            r'facebook\.com/login',
            r'login\.php',
            r'/login/',
            r'/auth/',
            r'accounts/login',
            r'\.html$',
            r'\.php$',
            r'\.jsp$',
            r'\.asp$'
        ]
        
        if any(re.search(pattern, url, re.IGNORECASE) for pattern in invalid_patterns):
            return False
        
        # URLs que provavelmente s√£o imagens
        valid_patterns = [
            r'\.(jpg|jpeg|png|gif|webp|bmp|svg)(\?|$)',
            r'scontent.*\.jpg',
            r'scontent.*\.png',
            r'cdninstagram\.com',
            r'fbcdn\.net',
            r'instagram\.com.*\.(jpg|png|webp)',
            r'facebook\.com.*\.(jpg|png|webp)',
            r'lookaside\.instagram\.com',  # URLs de widget/crawler do Instagram
            r'instagram\.com/seo/',        # URLs SEO do Instagram
            r'media_id=\d+',              # URLs com media_id (Instagram)
            r'graph\.instagram\.com',     # Graph API do Instagram
            r'img\.youtube\.com',         # Thumbnails do YouTube
            r'i\.ytimg\.com',            # Thumbnails alternativos do YouTube
            r'youtube\.com.*\.(jpg|png|webp)',  # Imagens do YouTube
            r'googleusercontent\.com',    # Imagens do Google
            r'ggpht\.com',               # Google Photos/YouTube
            r'ytimg\.com',               # YouTube images
            r'licdn\.com',               # LinkedIn CDN
            r'linkedin\.com.*\.(jpg|png|webp)',  # LinkedIn images
            r'sssinstagram\.com',        # SSS Instagram downloader
            r'scontent-.*\.cdninstagram\.com',  # Instagram CDN espec√≠fico
            r'scontent\..*\.fbcdn\.net'  # Facebook CDN espec√≠fico
        ]
        
        return any(re.search(pattern, url, re.IGNORECASE) for pattern in valid_patterns)

    async def _search_serper_advanced(self, query: str) -> List[Dict]:
        """Busca avan√ßada usando Serper com rota√ß√£o autom√°tica de APIs"""
        if not self.api_keys.get('serper'):
            return []
        results = []
        search_types = ['images', 'search']  # Busca por imagens e links
        for search_type in search_types:
            url = f"https://google.serper.dev/{search_type}"
            payload = {
                "q": query,
                "num": 20,  # Aumentar de 8 para 20
                "safe": "off",
                "gl": "br",
                "hl": "pt-br"
            }
            if search_type == 'images':
                payload.update({
                    "imgSize": "large",
                    "imgType": "photo",
                    "imgColorType": "color",
                    "imgDominantColor": "any"
                })
            # Tentar com rota√ß√£o de APIs
            api_key = self._get_next_api_key('serper')
            if not api_key:
                logger.error(f"‚ùå Nenhuma API Serper dispon√≠vel")
                continue
            headers = {
                'X-API-KEY': api_key,
                'Content-Type': 'application/json'
            }
            success = False
            for retry in range(len(self.api_keys['serper'])):
                try:
                    if HAS_ASYNC_DEPS:
                        timeout = aiohttp.ClientTimeout(total=self.config['timeout'])
                        async with aiohttp.ClientSession(timeout=timeout) as session:
                            async with session.post(url, headers=headers, json=payload) as response:
                                response.raise_for_status()
                                data = await response.json()
                    else:
                        response = self.session.post(url, headers=headers, json=payload, timeout=self.config['timeout'])
                        response.raise_for_status()
                        data = response.json()

                    if search_type == 'images':
                        for item in data.get('images', []):
                            results.append({
                                'image_url': item.get('imageUrl', ''),
                                'page_url': item.get('link', ''),
                                'title': item.get('title', ''),
                                'description': item.get('snippet', ''),
                                'source': 'serper_images'
                            })
                    else:  # search
                        for item in data.get('organic', []):
                            results.append({
                                'image_url': '',  # Ser√° extra√≠da depois
                                'page_url': item.get('link', ''),
                                'title': item.get('title', ''),
                                'description': item.get('snippet', ''),
                                'source': 'serper_search'
                            })

                    success = True
                    break

                except aiohttp.ClientError as e:
                    current_index = (self.current_api_index["serper"] - 1) % len(self.api_keys["serper"])
                    self._mark_api_failed("serper", current_index)
                    logger.error(f"‚ùå Erro de cliente Serper API #{current_index + 1}: {e}")
                    # Tentar pr√≥xima API
                    api_key = self._get_next_api_key("serper")
                    if api_key:
                        headers["X-API-KEY"] = api_key
                    else:
                        break

                except json.JSONDecodeError as e:
                    current_index = (self.current_api_index["serper"] - 1) % len(self.api_keys["serper"])
                    self._mark_api_failed("serper", current_index)
                    logger.error(f"‚ùå Erro de decodifica√ß√£o JSON Serper API #{current_index + 1}: {e}")
                    # Tentar pr√≥xima API
                    api_key = self._get_next_api_key("serper")
                    if api_key:
                        headers["X-API-KEY"] = api_key
                    else:
                        break

                except Exception as e:
                    current_index = (self.current_api_index["serper"] - 1) % len(self.api_keys["serper"])
                    self._mark_api_failed("serper", current_index)
                    logger.error(f"‚ùå Erro inesperado Serper API #{current_index + 1}: {e}")
                    # Tentar pr√≥xima API
                    api_key = self._get_next_api_key("serper")
                    if api_key:
                        headers["X-API-KEY"] = api_key
                    else:
                        break

                # Rate limiting entre tentativas com diferentes chaves
                await asyncio.sleep(0.3)

            # Verifica se alguma API funcionou
            if not success:
                logger.error(f"‚ùå Todas as APIs Serper falharam para {query}")
            # else:
            #     logger.info(f"‚úÖ Requisi√ß√£o Serper bem-sucedida para {query}") # Opcional

            # Rate limiting entre tipos de busca (images/search)
            await asyncio.sleep(0.3)

        return results # Retornar os resultados acumulados

    async def _search_google_cse_advanced(self, query: str) -> List[Dict]:
        """Busca aprimorada usando Google CSE"""
        if not self.config.get('google_search_key') or not self.config.get('google_cse_id'):
            return []
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            'key': self.config['google_search_key'],
            'cx': self.config['google_cse_id'],
            'q': query,
            'searchType': 'image',
            'num': 10,  # Aumentar de 6 para 10 (m√°ximo do Google CSE)
            'safe': 'off',
            'fileType': 'jpg,png,jpeg,webp,gif',
            'imgSize': 'large',
            'imgType': 'photo',
            'gl': 'br',
            'hl': 'pt'
        }
        try:
            if HAS_ASYNC_DEPS:
                timeout = aiohttp.ClientTimeout(total=self.config['timeout'])
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(url, params=params) as response:
                        response.raise_for_status()
                        data = await response.json()
            else:
                response = self.session.get(url, params=params, timeout=self.config['timeout'])
                response.raise_for_status()
                data = response.json()
            results = []
            for item in data.get('items', []):
                results.append({
                    'image_url': item.get('link', ''),
                    'page_url': item.get('image', {}).get('contextLink', ''),
                    'title': item.get('title', ''),
                    'description': item.get('snippet', ''),
                    'source': 'google_cse'
                })
            return results
        except Exception as e:
            if hasattr(e, 'response') and hasattr(e.response, 'status_code') and e.response.status_code == 429:
                logger.error(f"‚ùå Google CSE quota excedida")
            else:
                logger.error(f"‚ùå Erro na busca Google CSE: {e}")
            return []

    async def _search_rapidapi_instagram(self, query: str) -> List[Dict]:
        """Busca posts do Instagram via RapidAPI com rota√ß√£o autom√°tica"""
        if not self.api_keys.get('rapidapi'):
            return []
        url = "https://instagram-scraper-api2.p.rapidapi.com/v1/hashtag"
        params = {
            "hashtag": query.replace(' ', ''),
            "count": "12"
        }
        # Tentar com rota√ß√£o de APIs
        for attempt in range(len(self.api_keys['rapidapi'])):
            api_key = self._get_next_api_key('rapidapi')
            if not api_key:
                break
            headers = {
                "X-RapidAPI-Key": api_key,
                "X-RapidAPI-Host": "instagram-scraper-api2.p.rapidapi.com"
            }
            try:
                if HAS_ASYNC_DEPS:
                    timeout = aiohttp.ClientTimeout(total=30)
                    async with aiohttp.ClientSession(timeout=timeout) as session:
                        async with session.get(url, headers=headers, params=params) as response:
                            if response.status == 200:
                                data = await response.json()
                                results = []
                                # Ajuste na estrutura de dados do RapidAPI
                                for item in data.get('data', {}).get('recent', {}).get('sections', []):
                                    for media in item.get('layout_content', {}).get('medias', []):
                                        media_info = media.get('media', {})
                                        if media_info:
                                            results.append({
                                                'image_url': media_info.get('image_versions2', {}).get('candidates', [{}])[0].get('url', ''),
                                                'page_url': f"https://www.instagram.com/p/{media_info.get('code', '')}/",
                                                'title': f"Post do Instagram por @{media_info.get('user', {}).get('username', 'unknown')}",
                                                'description': media_info.get('caption', {}).get('text', '')[:200],
                                                'source': 'rapidapi_instagram'
                                            })
                                logger.info(f"‚úÖ RapidAPI sucesso: {len(results)} resultados")
                                return results
                            else:
                                raise Exception(f"Status {response.status}")
                else:
                    response = self.session.get(url, headers=headers, params=params, timeout=30)
                    if response.status_code == 200:
                        data = response.json()
                        # Similar parsing logic
                        return []
                    else:
                        raise Exception(f"Status {response.status_code}")
            except Exception as e:
                current_index = (self.current_api_index['rapidapi'] - 1) % len(self.api_keys['rapidapi'])
                self._mark_api_failed('rapidapi', current_index)
                logger.warning(f"‚ùå RapidAPI #{current_index + 1} falhou: {e}")
                continue
        logger.error(f"‚ùå Todas as APIs RapidAPI falharam")
        return []

    async def _search_youtube_thumbnails(self, query: str) -> List[Dict]:
        """Busca espec√≠fica por thumbnails do YouTube"""
        results = []
        youtube_queries = [
            f'"{query}" site:youtube.com',
            f'site:youtube.com/watch "{query}"',
            f'"{query}" youtube tutorial',
            f'"{query}" youtube curso',
            f'"{query}" youtube aula'
        ]
        
        for yt_query in youtube_queries[:3]:  # Limitar para evitar rate limit
            try:
                # Usar Serper para buscar v√≠deos do YouTube
                if self.api_keys.get('serper'):
                    api_key = self._get_next_api_key('serper')
                    if api_key:
                        url = "https://google.serper.dev/search"
                        payload = {
                            "q": yt_query,
                            "num": 15,
                            "safe": "off",
                            "gl": "br",
                            "hl": "pt-br"
                        }
                        headers = {
                            'X-API-KEY': api_key,
                            'Content-Type': 'application/json'
                        }
                        
                        if HAS_ASYNC_DEPS:
                            timeout = aiohttp.ClientTimeout(total=30)
                            async with aiohttp.ClientSession(timeout=timeout) as session:
                                async with session.post(url, json=payload, headers=headers) as response:
                                    if response.status == 200:
                                        data = await response.json()
                                        # Processar resultados do YouTube
                                        for item in data.get('organic', []):
                                            link = item.get('link', '')
                                            if 'youtube.com/watch' in link:
                                                # Extrair video ID e gerar thumbnail
                                                video_id = self._extract_youtube_id(link)
                                                if video_id:
                                                    # M√∫ltiplas qualidades de thumbnail
                                                    thumbnail_configs = [
                                                        ('maxresdefault.jpg', 'alta'),
                                                        ('hqdefault.jpg', 'm√©dia-alta'),
                                                        ('mqdefault.jpg', 'm√©dia'),
                                                        ('sddefault.jpg', 'padr√£o'),
                                                        ('default.jpg', 'baixa')
                                                    ]
                                                    for thumb_file, quality in thumbnail_configs:
                                                        thumb_url = f"https://img.youtube.com/vi/{video_id}/{thumb_file}"
                                                        results.append({
                                                            'image_url': thumb_url,
                                                            'page_url': link,
                                                            'title': f"{item.get('title', f'V√≠deo YouTube: {query}')} ({quality})",
                                                            'description': item.get('snippet', '')[:200],
                                                            'source': f'youtube_thumbnail_{quality}'
                                                        })
                        else:
                            response = self.session.post(url, json=payload, headers=headers, timeout=30)
                            if response.status_code == 200:
                                data = response.json()
                                # Similar processing for sync version
                                for item in data.get('organic', []):
                                    link = item.get('link', '')
                                    if 'youtube.com/watch' in link:
                                        video_id = self._extract_youtube_id(link)
                                        if video_id:
                                            # M√∫ltiplas qualidades de thumbnail
                                            thumbnail_configs = [
                                                ('maxresdefault.jpg', 'alta'),
                                                ('hqdefault.jpg', 'm√©dia-alta'),
                                                ('mqdefault.jpg', 'm√©dia')
                                            ]
                                            for thumb_file, quality in thumbnail_configs:
                                                thumb_url = f"https://img.youtube.com/vi/{video_id}/{thumb_file}"
                                                results.append({
                                                    'image_url': thumb_url,
                                                    'page_url': link,
                                                    'title': f"{item.get('title', f'V√≠deo YouTube: {query}')} ({quality})",
                                                    'description': item.get('snippet', '')[:200],
                                                    'source': f'youtube_thumbnail_{quality}'
                                                })
            except Exception as e:
                logger.warning(f"Erro na busca YouTube: {e}")
                continue
            
            await asyncio.sleep(0.3)  # Rate limiting
        
        logger.info(f"üì∫ YouTube encontrou {len(results)} thumbnails")
        return results

    def _extract_youtube_id(self, url: str) -> str:
        """Extrai ID do v√≠deo do YouTube da URL"""
        patterns = [
            r'youtube\.com/watch\?v=([^&]+)',
            r'youtu\.be/([^?]+)',
            r'youtube\.com/embed/([^?]+)'
        ]
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        return None

    async def _search_facebook_specific(self, query: str) -> List[Dict]:
        """Busca espec√≠fica para conte√∫do do Facebook"""
        results = []
        facebook_queries = [
            f'"{query}" site:facebook.com',
            f'site:facebook.com/posts "{query}"',
            f'site:facebook.com/photo "{query}"',
            f'"{query}" facebook curso',
            f'"{query}" facebook aula',
            f'"{query}" facebook dicas',
            f'site:facebook.com "{query}" tutorial'
        ]
        
        for fb_query in facebook_queries[:4]:  # Limitar para evitar rate limit
            try:
                # Usar Serper para buscar conte√∫do do Facebook
                if self.api_keys.get('serper'):
                    api_key = self._get_next_api_key('serper')
                    if api_key:
                        # Busca por imagens do Facebook
                        url = "https://google.serper.dev/images"
                        payload = {
                            "q": fb_query,
                            "num": 15,
                            "safe": "off",
                            "gl": "br",
                            "hl": "pt-br",
                            "imgSize": "large",
                            "imgType": "photo"
                        }
                        headers = {
                            'X-API-KEY': api_key,
                            'Content-Type': 'application/json'
                        }
                        
                        if HAS_ASYNC_DEPS:
                            timeout = aiohttp.ClientTimeout(total=30)
                            async with aiohttp.ClientSession(timeout=timeout) as session:
                                async with session.post(url, json=payload, headers=headers) as response:
                                    if response.status == 200:
                                        data = await response.json()
                                        # Processar resultados de imagens do Facebook
                                        for item in data.get('images', []):
                                            image_url = item.get('imageUrl', '')
                                            page_url = item.get('link', '')
                                            if image_url and ('facebook.com' in page_url or 'fbcdn.net' in image_url):
                                                results.append({
                                                    'image_url': image_url,
                                                    'page_url': page_url,
                                                    'title': item.get('title', f'Post Facebook: {query}'),
                                                    'description': item.get('snippet', '')[:200],
                                                    'source': 'facebook_image'
                                                })
                        else:
                            response = self.session.post(url, json=payload, headers=headers, timeout=30)
                            if response.status_code == 200:
                                data = response.json()
                                for item in data.get('images', []):
                                    image_url = item.get('imageUrl', '')
                                    page_url = item.get('link', '')
                                    if image_url and ('facebook.com' in page_url or 'fbcdn.net' in image_url):
                                        results.append({
                                            'image_url': image_url,
                                            'page_url': page_url,
                                            'title': item.get('title', f'Post Facebook: {query}'),
                                            'description': item.get('snippet', '')[:200],
                                            'source': 'facebook_image'
                                        })
            except Exception as e:
                logger.warning(f"Erro na busca Facebook espec√≠fica: {e}")
                continue
            
            await asyncio.sleep(0.3)  # Rate limiting
        
        logger.info(f"üìò Facebook espec√≠fico encontrou {len(results)} imagens")
        return results

    async def _search_alternative_strategies(self, query: str) -> List[Dict]:
        """Estrat√©gias alternativas de busca para aumentar resultados"""
        results = []
        
        # Estrat√©gias com termos mais amplos
        alternative_queries = [
            f'{query} tutorial',
            f'{query} curso',
            f'{query} aula',
            f'{query} dicas',
            f'{query} masterclass',
            f'{query} online',
            f'{query} gratis',
            f'{query} free',
            # Varia√ß√µes sem aspas para busca mais ampla
            f'{query} instagram',
            f'{query} facebook',
            f'{query} youtube',
            # Termos relacionados
            f'como {query}',
            f'aprenda {query}',
            f'{query} passo a passo'
        ]
        
        for alt_query in alternative_queries[:6]:  # Limitar para evitar rate limit
            try:
                if self.api_keys.get('serper'):
                    api_key = self._get_next_api_key('serper')
                    if api_key:
                        url = "https://google.serper.dev/images"
                        payload = {
                            "q": alt_query,
                            "num": 10,
                            "safe": "off",
                            "gl": "br",
                            "hl": "pt-br",
                            "imgSize": "medium",  # Usar medium para mais variedade
                            "imgType": "photo"
                        }
                        headers = {
                            'X-API-KEY': api_key,
                            'Content-Type': 'application/json'
                        }
                        
                        if HAS_ASYNC_DEPS:
                            timeout = aiohttp.ClientTimeout(total=30)
                            async with aiohttp.ClientSession(timeout=timeout) as session:
                                async with session.post(url, json=payload, headers=headers) as response:
                                    if response.status == 200:
                                        data = await response.json()
                                        for item in data.get('images', []):
                                            image_url = item.get('imageUrl', '')
                                            page_url = item.get('link', '')
                                            if image_url and self._is_valid_image_url(image_url):
                                                results.append({
                                                    'image_url': image_url,
                                                    'page_url': page_url,
                                                    'title': item.get('title', f'Conte√∫do: {query}'),
                                                    'description': item.get('snippet', '')[:200],
                                                    'source': 'alternative_search'
                                                })
                        else:
                            response = self.session.post(url, json=payload, headers=headers, timeout=30)
                            if response.status_code == 200:
                                data = response.json()
                                for item in data.get('images', []):
                                    image_url = item.get('imageUrl', '')
                                    page_url = item.get('link', '')
                                    if image_url and self._is_valid_image_url(image_url):
                                        results.append({
                                            'image_url': image_url,
                                            'page_url': page_url,
                                            'title': item.get('title', f'Conte√∫do: {query}'),
                                            'description': item.get('snippet', '')[:200],
                                            'source': 'alternative_search'
                                        })
            except Exception as e:
                logger.warning(f"Erro na busca alternativa: {e}")
                continue
            
            await asyncio.sleep(0.2)  # Rate limiting mais r√°pido
        
        logger.info(f"üîÑ Estrat√©gias alternativas encontraram {len(results)} imagens")
        return results

    async def _download_real_image(self, image_url: str, post_url: str, source: str) -> Optional[str]:
        """Baixa uma imagem real e salva no disco"""
        
        # VALIDA√á√ÉO ULTRA-RIGOROSA: Rejeitar URLs de CDN imediatamente
        cdn_patterns = [
            'lookaside.instagram.com', 'scontent-', 'cdninstagram', 
            'fbcdn', 'google_widget', 'crawler', '/seo/', 'media_id='
        ]
        
        for pattern in cdn_patterns:
            if pattern in image_url.lower():
                logger.warning(f"‚ö†Ô∏è URL de CDN rejeitada no download: {image_url}")
                return None
        
        try:
            # Gerar nome √∫nico para o arquivo
            url_hash = hashlib.md5(image_url.encode()).hexdigest()[:8]
            timestamp = int(time.time())
            
            # Detectar extens√£o da imagem
            extension = 'jpg'
            if '.png' in image_url.lower():
                extension = 'png'
            elif '.webp' in image_url.lower():
                extension = 'webp'
            elif '.gif' in image_url.lower():
                extension = 'gif'
            
            filename = f"instagram_real_{source}_{url_hash}_{timestamp}.{extension}"
            filepath = os.path.join(self.config['images_dir'], filename)
            
            # Headers para simular navegador real
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.9',
                'Referer': 'https://www.instagram.com/',
                'Sec-Fetch-Dest': 'image',
                'Sec-Fetch-Mode': 'no-cors',
                'Sec-Fetch-Site': 'cross-site'
            }
            
            if HAS_ASYNC_DEPS:
                timeout = aiohttp.ClientTimeout(total=30)
                async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
                    async with session.get(image_url) as response:
                        if response.status == 200:
                            content = await response.read()
                            
                            # Verificar se √© realmente uma imagem
                            if len(content) > 1000 and content[:4] in [b'\xff\xd8\xff', b'\x89PNG', b'RIFF', b'GIF8']:
                                with open(filepath, 'wb') as f:
                                    f.write(content)
                                
                                logger.info(f"‚úÖ Imagem real baixada: {filename}")
                                return filepath
                            else:
                                logger.warning(f"‚ö†Ô∏è Conte√∫do n√£o √© uma imagem v√°lida: {image_url}")
                                return None
            else:
                response = self.session.get(image_url, headers=headers, timeout=30)
                if response.status_code == 200:
                    content = response.content
                    
                    # Verificar se √© realmente uma imagem
                    if len(content) > 1000 and content[:4] in [b'\xff\xd8\xff', b'\x89PNG', b'RIFF', b'GIF8']:
                        with open(filepath, 'wb') as f:
                            f.write(content)
                        
                        logger.info(f"‚úÖ Imagem real baixada: {filename}")
                        return filepath
                    else:
                        logger.warning(f"‚ö†Ô∏è Conte√∫do n√£o √© uma imagem v√°lida: {image_url}")
                        return None
                        
        except Exception as e:
            logger.error(f"‚ùå Erro no download da imagem {image_url}: {e}")
            return None
        
        return None

    async def _extract_instagram_direct(self, post_url: str) -> List[Dict]:
        """Extrai imagens do Instagram APENAS via sssinstagram (foco em marketing)"""
        results = []
        
        try:
            # √öNICA ESTRAT√âGIA: sssinstagram (mais confi√°vel para marketing)
            results_sss = await self._extract_via_sssinstagram_only(post_url)
            results.extend(results_sss)
            
            # Se n√£o conseguiu, tentar oembed b√°sico (sem screenshot)
            if len(results) == 0:
                results_oembed = await self._extract_instagram_oembed_basic(post_url)
                results.extend(results_oembed)
                
        except Exception as e:
            logger.error(f"‚ùå Erro na extra√ß√£o Instagram para marketing: {e}")
        
        logger.info(f"üéØ Instagram marketing: {len(results)} imagens de alta qualidade extra√≠das")
        return results

    async def _extract_instagram_oembed_fallback(self, post_url: str) -> List[Dict]:
        """Fallback usando oembed do Instagram quando APIs falham"""
        results = []
        
        try:
            # Extrair ID do post da URL
            post_id = None
            if '/p/' in post_url:
                post_id = post_url.split('/p/')[1].split('/')[0]
            elif '/reel/' in post_url:
                post_id = post_url.split('/reel/')[1].split('/')[0]
            
            if not post_id:
                return results
            
            # FALLBACK ROBUSTO: Criar resultado b√°sico sem depender de APIs externas
            logger.info(f"‚úÖ Criando resultado b√°sico para: {post_url}")
            
            # Extrair informa√ß√µes b√°sicas da URL
            post_type = 'post'
            if '/reel/' in post_url:
                post_type = 'reel'
            elif '/tv/' in post_url:
                post_type = 'igtv'
            
            # Criar resultado b√°sico garantido
            result = {
                'image_url': f"https://www.instagram.com/p/{post_id}/media/?size=l",  # URL padr√£o do Instagram
                'downloaded_path': None,
                'platform': 'instagram',
                'content_type': f'{post_type}_fallback',
                'source': 'instagram_basic_fallback',
                'post_url': post_url,
                'quality_score': 6,  # Score bom para fallback
                'marketing_insights': {
                    'engagement_potential': 'medium',
                    'conversion_indicators': ['instagram_post'],
                    'ad_potential': 'medium',
                    'audience_targeting': ['instagram_users', 'social_media']
                },
                'title': f'Instagram {post_type.title()} - {post_id}',
                'author_name': 'Instagram User',
                'width': 640,
                'height': 640,
                'post_id': post_id,
                'post_type': post_type
            }
            
            results.append(result)
            logger.info(f"‚úÖ Fallback b√°sico criado com sucesso para {post_url}")
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Fallback oembed falhou: {e}")
        
        return results

    def _is_valid_instagram_post_url(self, url: str) -> bool:
        """Valida se √© uma URL direta de post do Instagram (aceita pelo sssinstagram)"""
        if not url:
            return False
            
        # Padr√µes de URLs V√ÅLIDAS para sssinstagram
        valid_patterns = [
            r'https?://(?:www\.)?instagram\.com/p/[A-Za-z0-9_-]+/?',      # Posts normais
            r'https?://(?:www\.)?instagram\.com/reel/[A-Za-z0-9_-]+/?',   # Reels
            r'https?://(?:www\.)?instagram\.com/tv/[A-Za-z0-9_-]+/?',     # IGTV
        ]
        
        # Padr√µes INV√ÅLIDOS (CDN, SEO, etc.)
        invalid_patterns = [
            r'lookaside\.instagram\.com',
            r'scontent.*\.cdninstagram\.com',
            r'scontent.*\.fbcdn\.net',
            r'media_id=',
            r'/seo/',
            r'graph\.instagram\.com',
            r'\.jpg',
            r'\.png',
            r'\.webp',
            r'\.gif'
        ]
        
        # Verificar se √© inv√°lida
        for pattern in invalid_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return False
        
        # Verificar se √© v√°lida
        for pattern in valid_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return True
                
        return False

    def _is_valid_facebook_post_url(self, url: str) -> bool:
        """Valida se √© uma URL direta de post do Facebook (aceita pelo hitube.io)"""
        if not url:
            return False
            
        # Padr√µes de URLs V√ÅLIDAS para hitube.io
        valid_patterns = [
            r'https?://(?:www\.)?facebook\.com/[^/]+/posts/[0-9]+',       # Posts normais
            r'https?://(?:www\.)?facebook\.com/[^/]+/videos/[0-9]+',      # V√≠deos
            r'https?://(?:www\.)?facebook\.com/photo\.php\?fbid=[0-9]+',  # Fotos
            r'https?://(?:www\.)?facebook\.com/watch/?\?v=[0-9]+',        # Watch
            r'https?://(?:www\.)?facebook\.com/reel/[0-9]+',              # Reels
        ]
        
        # Padr√µes INV√ÅLIDOS (CDN, etc.)
        invalid_patterns = [
            r'scontent.*\.fbcdn\.net',
            r'fbcdn\.net',
            r'\.jpg',
            r'\.png',
            r'\.webp',
            r'\.gif',
            r'\.mp4'
        ]
        
        # Verificar se √© inv√°lida
        for pattern in invalid_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return False
        
        # Verificar se √© v√°lida
        for pattern in valid_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return True
                
        return False

    def _is_valid_youtube_url(self, url: str) -> bool:
        """Valida se √© uma URL direta de v√≠deo do YouTube"""
        if not url:
            return False
            
        valid_patterns = [
            r'https?://(?:www\.)?youtube\.com/watch\?v=[A-Za-z0-9_-]+',
            r'https?://youtu\.be/[A-Za-z0-9_-]+',
            r'https?://(?:www\.)?youtube\.com/embed/[A-Za-z0-9_-]+',
        ]
        
        return any(re.search(pattern, url, re.IGNORECASE) for pattern in valid_patterns)

    def _is_valid_linkedin_url(self, url: str) -> bool:
        """Valida se √© uma URL direta de post do LinkedIn"""
        if not url:
            return False
            
        valid_patterns = [
            r'https?://(?:www\.)?linkedin\.com/posts/[^/]+_[A-Za-z0-9_-]+',
            r'https?://(?:www\.)?linkedin\.com/feed/update/[A-Za-z0-9:_-]+',
            r'https?://(?:www\.)?linkedin\.com/pulse/[^/]+',
        ]
        
        invalid_patterns = [
            r'media\.licdn\.com',
            r'\.jpg',
            r'\.png',
            r'\.webp',
            r'\.gif'
        ]
        
        # Verificar se √© inv√°lida
        for pattern in invalid_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return False
        
        return any(re.search(pattern, url, re.IGNORECASE) for pattern in valid_patterns)

    def _is_valid_tiktok_url(self, url: str) -> bool:
        """Valida se √© uma URL direta de v√≠deo do TikTok"""
        if not url:
            return False
            
        valid_patterns = [
            r'https?://(?:www\.)?tiktok\.com/@[^/]+/video/[0-9]+',
            r'https?://vm\.tiktok\.com/[A-Za-z0-9]+',
            r'https?://(?:www\.)?tiktok\.com/t/[A-Za-z0-9]+',
        ]
        
        invalid_patterns = [
            r'\.mp4',
            r'\.jpg',
            r'\.png',
            r'\.webp',
            r'\.gif'
        ]
        
        # Verificar se √© inv√°lida
        for pattern in invalid_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return False
        
        return any(re.search(pattern, url, re.IGNORECASE) for pattern in valid_patterns)

    async def _extract_via_sssinstagram_only(self, post_url: str) -> List[Dict]:
        """Extrai imagens APENAS via sssinstagram - foco em marketing digital"""
        results = []
        
        try:
            # FILTRAR URL: S√≥ aceitar URLs diretas do Instagram
            if not self._is_valid_instagram_post_url(post_url):
                logger.warning(f"‚ö†Ô∏è URL n√£o √© um post direto do Instagram: {post_url}")
                return results
            
            logger.info(f"üéØ Usando sssinstagram para marketing: {post_url}")
            
            sss_config = {
                'name': 'sssinstagram',
                'url': 'https://sssinstagram.com/api/ig/post',
                'method': 'POST',
                'payload': {'url': post_url},
                'headers': {
                    'Content-Type': 'application/json',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Referer': 'https://sssinstagram.com/',
                    'Accept': 'application/json, text/plain, */*',
                    'Accept-Language': 'en-US,en;q=0.9'
                }
            }
            
            # TENTATIVA 1: API sssinstagram
            try:
                if HAS_ASYNC_DEPS:
                    timeout = aiohttp.ClientTimeout(total=15)  # Timeout reduzido
                    async with aiohttp.ClientSession(timeout=timeout, headers=sss_config['headers']) as session:
                        async with session.post(sss_config['url'], json=sss_config['payload']) as response:
                            if response.status == 200:
                                content_type = response.headers.get('content-type', '')
                                if 'application/json' in content_type:
                                    data = await response.json()
                                    processed_results = await self._process_sssinstagram_marketing(data, post_url)
                                    results.extend(processed_results)
                                else:
                                    logger.warning(f"‚ö†Ô∏è sssinstagram retornou HTML em vez de JSON")
                            else:
                                logger.warning(f"‚ö†Ô∏è sssinstagram retornou status {response.status}")
                else:
                    response = self.session.post(sss_config['url'], json=sss_config['payload'], headers=sss_config['headers'], timeout=15)
                    if response.status_code == 200:
                        content_type = response.headers.get('content-type', '')
                        if 'application/json' in content_type:
                            data = response.json()
                            processed_results = await self._process_sssinstagram_marketing(data, post_url)
                            results.extend(processed_results)
                        else:
                            logger.warning(f"‚ö†Ô∏è sssinstagram retornou HTML em vez de JSON")
                    else:
                        logger.warning(f"‚ö†Ô∏è sssinstagram retornou status {response.status_code}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro na API sssinstagram: {e}")
            
            # FALLBACK: Se sssinstagram falhou, usar oembed b√°sico
            if len(results) == 0:
                try:
                    fallback_results = await self._extract_instagram_oembed_fallback(post_url)
                    results.extend(fallback_results)
                    if len(fallback_results) > 0:
                        logger.info(f"‚úÖ Fallback oembed funcionou: {len(fallback_results)} itens")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Fallback oembed tamb√©m falhou: {e}")
                    
        except Exception as e:
            logger.error(f"‚ùå sssinstagram falhou: {e}")
        
        return results

    async def _process_sssinstagram_marketing(self, data: dict, post_url: str) -> List[Dict]:
        """Processa resposta do sssinstagram com foco em marketing"""
        results = []
        
        try:
            if data.get('success') and data.get('data'):
                media_data = data['data']
                
                # Extrair informa√ß√µes de marketing relevantes
                engagement_data = self._extract_marketing_insights(data)
                
                if isinstance(media_data, list):
                    for item in media_data:
                        if item.get('url'):
                            # Baixar imagem real de alta qualidade
                            downloaded_path = await self._download_real_image(item['url'], post_url, 'sssinstagram')
                            
                            result = {
                                'image_url': item['url'],
                                'downloaded_path': downloaded_path,
                                'page_url': post_url,
                                'title': f'Instagram Marketing Content',
                                'description': item.get('caption', '')[:300],  # Mais texto para an√°lise
                                'source': 'sssinstagram_marketing',
                                'marketing_insights': engagement_data,
                                'content_type': self._detect_content_type(item),
                                'quality_score': self._calculate_quality_score(item)
                            }
                            results.append(result)
                            
                elif media_data.get('url'):
                    downloaded_path = await self._download_real_image(media_data['url'], post_url, 'sssinstagram')
                    
                    result = {
                        'image_url': media_data['url'],
                        'downloaded_path': downloaded_path,
                        'page_url': post_url,
                        'title': f'Instagram Marketing Content',
                        'description': media_data.get('caption', '')[:300],
                        'source': 'sssinstagram_marketing',
                        'marketing_insights': engagement_data,
                        'content_type': self._detect_content_type(media_data),
                        'quality_score': self._calculate_quality_score(media_data)
                    }
                    results.append(result)
                    
        except Exception as e:
            logger.warning(f"Erro processando sssinstagram marketing: {e}")
        
        return results

    def _extract_marketing_insights(self, data: dict) -> dict:
        """Extrai insights de marketing dos dados"""
        insights = {
            'engagement_potential': 'high',  # Baseado na fonte ser viral
            'content_format': 'unknown',
            'estimated_reach': 'unknown',
            'conversion_indicators': []
        }
        
        # Analisar indicadores de convers√£o no conte√∫do
        if data.get('data'):
            content = str(data['data']).lower()
            
            # Indicadores de alta convers√£o
            conversion_keywords = [
                'link in bio', 'swipe up', 'dm me', 'comment below',
                'tag a friend', 'save this post', 'share if you agree',
                'limited time', 'exclusive', 'discount', 'promo',
                'free', 'giveaway', 'contest', 'winner'
            ]
            
            for keyword in conversion_keywords:
                if keyword in content:
                    insights['conversion_indicators'].append(keyword)
            
            # Determinar potencial de engajamento
            if len(insights['conversion_indicators']) > 2:
                insights['engagement_potential'] = 'very_high'
            elif len(insights['conversion_indicators']) > 0:
                insights['engagement_potential'] = 'high'
        
        return insights

    def _detect_content_type(self, item: dict) -> str:
        """Detecta o tipo de conte√∫do para an√°lise de marketing"""
        if 'video' in str(item).lower():
            return 'video'
        elif 'carousel' in str(item).lower():
            return 'carousel'
        elif 'story' in str(item).lower():
            return 'story'
        else:
            return 'image'

    def _calculate_quality_score(self, item: dict) -> int:
        """Calcula score de qualidade para marketing (1-10)"""
        score = 5  # Base
        
        # Verificar resolu√ß√£o/qualidade
        if item.get('width', 0) > 1080:
            score += 2
        elif item.get('width', 0) > 720:
            score += 1
            
        # Verificar se tem caption/descri√ß√£o
        if item.get('caption') and len(item['caption']) > 50:
            score += 1
            
        # Verificar indicadores de qualidade
        if 'hd' in str(item).lower() or 'high' in str(item).lower():
            score += 1
            
        return min(score, 10)

    async def _extract_instagram_oembed_basic(self, post_url: str) -> List[Dict]:
        """Extra√ß√£o b√°sica via oembed (sem screenshot) para backup"""
        results = []
        
        try:
            oembed_url = f'https://www.instagram.com/api/v1/oembed/?url={post_url}'
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'application/json'
            }
            
            if HAS_ASYNC_DEPS:
                timeout = aiohttp.ClientTimeout(total=15)
                async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
                    async with session.get(oembed_url) as response:
                        if response.status == 200:
                            data = await response.json()
                            if data.get('thumbnail_url'):
                                downloaded_path = await self._download_real_image(data['thumbnail_url'], post_url, 'oembed')
                                results.append({
                                    'image_url': data['thumbnail_url'],
                                    'downloaded_path': downloaded_path,
                                    'page_url': post_url,
                                    'title': data.get('title', 'Instagram Content'),
                                    'description': data.get('author_name', ''),
                                    'source': 'instagram_oembed_backup',
                                    'marketing_insights': {'engagement_potential': 'medium'},
                                    'content_type': 'image',
                                    'quality_score': 6
                                })
            else:
                response = self.session.get(oembed_url, headers=headers, timeout=15)
                if response.status_code == 200:
                    data = response.json()
                    if data.get('thumbnail_url'):
                        downloaded_path = await self._download_real_image(data['thumbnail_url'], post_url, 'oembed')
                        results.append({
                            'image_url': data['thumbnail_url'],
                            'downloaded_path': downloaded_path,
                            'page_url': post_url,
                            'title': data.get('title', 'Instagram Content'),
                            'description': data.get('author_name', ''),
                            'source': 'instagram_oembed_backup',
                            'marketing_insights': {'engagement_potential': 'medium'},
                            'content_type': 'image',
                            'quality_score': 6
                        })
                        
        except Exception as e:
            logger.warning(f"Backup oembed falhou: {e}")
        
        return results

    async def _extract_facebook_marketing(self, post_url: str) -> List[Dict]:
        """Extrai conte√∫do do Facebook usando hitube.io para marketing"""
        results = []
        
        try:
            # FILTRAR URL: S√≥ aceitar URLs diretas do Facebook
            if not self._is_valid_facebook_post_url(post_url):
                logger.warning(f"‚ö†Ô∏è URL n√£o √© um post direto do Facebook: {post_url}")
                return results
            
            logger.info(f"üéØ Extraindo Facebook para marketing: {post_url}")
            
            # Configura√ß√£o para hitube.io
            hitube_config = {
                'name': 'hitube_facebook',
                'url': 'https://www.hitube.io/api/facebook/download',
                'method': 'POST',
                'payload': {'url': post_url, 'format': 'json'},
                'headers': {
                    'Content-Type': 'application/json',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Referer': 'https://www.hitube.io/pt/facebook-video-download',
                    'Accept': 'application/json, text/plain, */*',
                    'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8'
                }
            }
            
            # TENTATIVA 1: API hitube.io
            try:
                if HAS_ASYNC_DEPS:
                    timeout = aiohttp.ClientTimeout(total=15)
                    async with aiohttp.ClientSession(timeout=timeout, headers=hitube_config['headers']) as session:
                        async with session.post(hitube_config['url'], json=hitube_config['payload']) as response:
                            if response.status == 200:
                                content_type = response.headers.get('content-type', '')
                                if 'application/json' in content_type:
                                    data = await response.json()
                                    processed_results = await self._process_facebook_marketing(data, post_url)
                                    results.extend(processed_results)
                                else:
                                    logger.warning(f"‚ö†Ô∏è hitube.io retornou HTML em vez de JSON")
                            else:
                                logger.warning(f"‚ö†Ô∏è hitube.io retornou status {response.status}")
                else:
                    response = self.session.post(hitube_config['url'], json=hitube_config['payload'], headers=hitube_config['headers'], timeout=15)
                    if response.status_code == 200:
                        content_type = response.headers.get('content-type', '')
                        if 'application/json' in content_type:
                            data = response.json()
                            processed_results = await self._process_facebook_marketing(data, post_url)
                            results.extend(processed_results)
                        else:
                            logger.warning(f"‚ö†Ô∏è hitube.io retornou HTML em vez de JSON")
                    else:
                        logger.warning(f"‚ö†Ô∏è hitube.io retornou status {response.status_code}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Erro na API hitube.io: {e}")
            
            # FALLBACK: Se hitube.io falhou, usar oembed b√°sico do Facebook
            if len(results) == 0:
                try:
                    fallback_results = await self._extract_facebook_oembed_fallback(post_url)
                    results.extend(fallback_results)
                    if len(fallback_results) > 0:
                        logger.info(f"‚úÖ Fallback Facebook oembed funcionou: {len(fallback_results)} itens")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Fallback Facebook oembed tamb√©m falhou: {e}")
                    
        except Exception as e:
            logger.error(f"‚ùå Extra√ß√£o Facebook falhou: {e}")
        
        logger.info(f"üìò Facebook marketing: {len(results)} conte√∫dos extra√≠dos")
        return results

    async def _extract_facebook_oembed_fallback(self, post_url: str) -> List[Dict]:
        """Fallback usando oembed do Facebook quando APIs falham"""
        results = []
        
        try:
            # Usar oembed do Facebook
            oembed_url = f"https://www.facebook.com/plugins/post/oembed.json/?url={post_url}"
            
            if HAS_ASYNC_DEPS:
                timeout = aiohttp.ClientTimeout(total=10)
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(oembed_url) as response:
                        if response.status == 200:
                            data = await response.json()
                            
                            # Criar resultado b√°sico com dados do oembed
                            result = {
                                'image_url': '',  # Facebook oembed n√£o fornece imagem direta
                                'downloaded_path': None,
                                'platform': 'facebook',
                                'content_type': 'oembed_fallback',
                                'source': 'facebook_oembed',
                                'post_url': post_url,
                                'quality_score': 5,  # Score m√©dio para fallback
                                'marketing_insights': {
                                    'engagement_potential': 'medium',
                                    'conversion_indicators': [],
                                    'ad_potential': 'medium',
                                    'audience_targeting': ['facebook_users']
                                },
                                'title': data.get('title', ''),
                                'author_name': data.get('author_name', ''),
                                'width': data.get('width', 0),
                                'height': data.get('height', 0),
                                'html': data.get('html', '')  # Embed HTML do Facebook
                            }
                            
                            results.append(result)
                            
            else:
                response = self.session.get(oembed_url, timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    
                    result = {
                        'image_url': '',
                        'downloaded_path': None,
                        'platform': 'facebook',
                        'content_type': 'oembed_fallback',
                        'source': 'facebook_oembed',
                        'post_url': post_url,
                        'quality_score': 5,
                        'marketing_insights': {
                            'engagement_potential': 'medium',
                            'conversion_indicators': [],
                            'ad_potential': 'medium',
                            'audience_targeting': ['facebook_users']
                        },
                        'title': data.get('title', ''),
                        'author_name': data.get('author_name', ''),
                        'width': data.get('width', 0),
                        'height': data.get('height', 0),
                        'html': data.get('html', '')
                    }
                    
                    results.append(result)
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Fallback Facebook oembed falhou: {e}")
        
        return results

    async def _process_facebook_marketing(self, data: dict, post_url: str) -> List[Dict]:
        """Processa resposta do Facebook com foco em marketing"""
        results = []
        
        try:
            if data.get('success') and data.get('data'):
                media_data = data['data']
                
                # Extrair insights de marketing do Facebook
                fb_insights = self._extract_facebook_marketing_insights(data)
                
                if isinstance(media_data, list):
                    for item in media_data:
                        if item.get('download_url') or item.get('video_url') or item.get('image_url'):
                            media_url = item.get('download_url') or item.get('video_url') or item.get('image_url')
                            
                            # Baixar conte√∫do real
                            downloaded_path = await self._download_real_image(media_url, post_url, 'facebook')
                            
                            result = {
                                'image_url': media_url,
                                'downloaded_path': downloaded_path,
                                'page_url': post_url,
                                'title': f'Facebook Marketing Content',
                                'description': item.get('description', item.get('title', ''))[:300],
                                'source': 'facebook_hitube_marketing',
                                'marketing_insights': fb_insights,
                                'content_type': self._detect_facebook_content_type(item),
                                'quality_score': self._calculate_facebook_quality_score(item),
                                'platform': 'facebook'
                            }
                            results.append(result)
                            
                elif media_data.get('download_url') or media_data.get('video_url') or media_data.get('image_url'):
                    media_url = media_data.get('download_url') or media_data.get('video_url') or media_data.get('image_url')
                    downloaded_path = await self._download_real_image(media_url, post_url, 'facebook')
                    
                    result = {
                        'image_url': media_url,
                        'downloaded_path': downloaded_path,
                        'page_url': post_url,
                        'title': f'Facebook Marketing Content',
                        'description': media_data.get('description', media_data.get('title', ''))[:300],
                        'source': 'facebook_hitube_marketing',
                        'marketing_insights': fb_insights,
                        'content_type': self._detect_facebook_content_type(media_data),
                        'quality_score': self._calculate_facebook_quality_score(media_data),
                        'platform': 'facebook'
                    }
                    results.append(result)
                    
        except Exception as e:
            logger.warning(f"Erro processando Facebook marketing: {e}")
        
        return results

    def _extract_facebook_marketing_insights(self, data: dict) -> dict:
        """Extrai insights de marketing espec√≠ficos do Facebook"""
        insights = {
            'engagement_potential': 'high',
            'content_format': 'unknown',
            'estimated_reach': 'unknown',
            'conversion_indicators': [],
            'ad_potential': 'medium',
            'audience_targeting': []
        }
        
        # Analisar conte√∫do para indicadores de marketing
        if data.get('data'):
            content = str(data['data']).lower()
            
            # Indicadores espec√≠ficos do Facebook
            fb_conversion_keywords = [
                'learn more', 'shop now', 'sign up', 'get started',
                'download', 'install', 'try now', 'book now',
                'call now', 'message us', 'contact us', 'visit website',
                'limited offer', 'sale', 'discount', 'promotion',
                'sponsored', 'ad', 'advertisement'
            ]
            
            for keyword in fb_conversion_keywords:
                if keyword in content:
                    insights['conversion_indicators'].append(keyword)
            
            # Detectar potencial de an√∫ncio
            ad_indicators = ['sponsored', 'promoted', 'ad', 'advertisement', 'boost']
            if any(indicator in content for indicator in ad_indicators):
                insights['ad_potential'] = 'very_high'
            
            # Determinar potencial de engajamento
            if len(insights['conversion_indicators']) > 3:
                insights['engagement_potential'] = 'very_high'
            elif len(insights['conversion_indicators']) > 1:
                insights['engagement_potential'] = 'high'
        
        return insights

    def _detect_facebook_content_type(self, item: dict) -> str:
        """Detecta tipo de conte√∫do do Facebook"""
        content_str = str(item).lower()
        
        if 'video' in content_str:
            return 'video'
        elif 'reel' in content_str:
            return 'reel'
        elif 'story' in content_str:
            return 'story'
        elif 'live' in content_str:
            return 'live'
        elif 'carousel' in content_str or 'album' in content_str:
            return 'carousel'
        else:
            return 'image'

    def _calculate_facebook_quality_score(self, item: dict) -> int:
        """Calcula score de qualidade para conte√∫do do Facebook"""
        score = 5  # Base
        
        # Verificar qualidade de v√≠deo/imagem
        if item.get('quality') == 'hd' or item.get('resolution', '').startswith('1080'):
            score += 2
        elif item.get('quality') == 'sd' or '720' in str(item.get('resolution', '')):
            score += 1
            
        # Verificar dura√ß√£o (para v√≠deos)
        duration = item.get('duration', 0)
        if isinstance(duration, (int, float)):
            if 15 <= duration <= 60:  # Dura√ß√£o ideal para engajamento
                score += 1
            elif 60 < duration <= 180:
                score += 0.5
                
        # Verificar descri√ß√£o/t√≠tulo
        description = item.get('description', '') or item.get('title', '')
        if len(description) > 50:
            score += 1
            
        # Verificar indicadores de qualidade
        if any(keyword in str(item).lower() for keyword in ['hd', 'high quality', '1080p', '4k']):
            score += 1
            
        return min(int(score), 10)

    async def _extract_youtube_marketing(self, video_url: str) -> List[Dict]:
        """Extrai thumbnails do YouTube usando youtube-thumbnail-grabber.com"""
        results = []
        
        try:
            # FILTRAR URL: S√≥ aceitar URLs diretas do YouTube
            if not self._is_valid_youtube_url(video_url):
                logger.warning(f"‚ö†Ô∏è URL n√£o √© um v√≠deo direto do YouTube: {video_url}")
                return results
            
            logger.info(f"üéØ Extraindo YouTube para marketing: {video_url}")
            
            # Configura√ß√£o para youtube-thumbnail-grabber.com
            yt_config = {
                'name': 'youtube_thumbnail_grabber',
                'url': 'https://youtube-thumbnail-grabber.com/api/thumbnail',
                'method': 'POST',
                'payload': {'url': video_url, 'quality': 'maxresdefault'},
                'headers': {
                    'Content-Type': 'application/json',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Referer': 'https://youtube-thumbnail-grabber.com/',
                    'Accept': 'application/json, text/plain, */*'
                }
            }
            
            if HAS_ASYNC_DEPS:
                timeout = aiohttp.ClientTimeout(total=30)
                async with aiohttp.ClientSession(timeout=timeout, headers=yt_config['headers']) as session:
                    async with session.post(yt_config['url'], json=yt_config['payload']) as response:
                        if response.status == 200:
                            data = await response.json()
                            processed_results = await self._process_youtube_marketing(data, video_url)
                            results.extend(processed_results)
                        else:
                            logger.warning(f"‚ö†Ô∏è youtube-thumbnail-grabber retornou status {response.status}")
            else:
                response = self.session.post(yt_config['url'], json=yt_config['payload'], headers=yt_config['headers'], timeout=30)
                if response.status_code == 200:
                    data = response.json()
                    processed_results = await self._process_youtube_marketing(data, video_url)
                    results.extend(processed_results)
                else:
                    logger.warning(f"‚ö†Ô∏è youtube-thumbnail-grabber retornou status {response.status_code}")
                    
        except Exception as e:
            logger.error(f"‚ùå Extra√ß√£o YouTube falhou: {e}")
        
        logger.info(f"üì∫ YouTube marketing: {len(results)} thumbnails extra√≠dos")
        return results

    async def _extract_linkedin_marketing(self, post_url: str) -> List[Dict]:
        """Extrai imagens do LinkedIn usando linkedindownloader.io"""
        results = []
        
        try:
            # FILTRAR URL: S√≥ aceitar URLs diretas do LinkedIn
            if not self._is_valid_linkedin_url(post_url):
                logger.warning(f"‚ö†Ô∏è URL n√£o √© um post direto do LinkedIn: {post_url}")
                return results
            
            logger.info(f"üéØ Extraindo LinkedIn para marketing: {post_url}")
            
            # Configura√ß√£o para linkedindownloader.io
            li_config = {
                'name': 'linkedin_downloader',
                'url': 'https://linkedindownloader.io/api/download',
                'method': 'POST',
                'payload': {'url': post_url, 'type': 'image'},
                'headers': {
                    'Content-Type': 'application/json',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Referer': 'https://linkedindownloader.io/linkedin-image-downloader/',
                    'Accept': 'application/json, text/plain, */*'
                }
            }
            
            if HAS_ASYNC_DEPS:
                timeout = aiohttp.ClientTimeout(total=30)
                async with aiohttp.ClientSession(timeout=timeout, headers=li_config['headers']) as session:
                    async with session.post(li_config['url'], json=li_config['payload']) as response:
                        if response.status == 200:
                            data = await response.json()
                            processed_results = await self._process_linkedin_marketing(data, post_url)
                            results.extend(processed_results)
                        else:
                            logger.warning(f"‚ö†Ô∏è linkedindownloader.io retornou status {response.status}")
            else:
                response = self.session.post(li_config['url'], json=li_config['payload'], headers=li_config['headers'], timeout=30)
                if response.status_code == 200:
                    data = response.json()
                    processed_results = await self._process_linkedin_marketing(data, post_url)
                    results.extend(processed_results)
                else:
                    logger.warning(f"‚ö†Ô∏è linkedindownloader.io retornou status {response.status_code}")
                    
        except Exception as e:
            logger.error(f"‚ùå Extra√ß√£o LinkedIn falhou: {e}")
        
        logger.info(f"üíº LinkedIn marketing: {len(results)} imagens extra√≠das")
        return results

    async def _extract_tiktok_marketing(self, video_url: str) -> List[Dict]:
        """Extrai thumbnails do TikTok usando tiktok.coderobo.org"""
        results = []
        
        try:
            # FILTRAR URL: S√≥ aceitar URLs diretas do TikTok
            if not self._is_valid_tiktok_url(video_url):
                logger.warning(f"‚ö†Ô∏è URL n√£o √© um v√≠deo direto do TikTok: {video_url}")
                return results
            
            logger.info(f"üéØ Extraindo TikTok para marketing: {video_url}")
            
            # Configura√ß√£o para tiktok.coderobo.org
            tt_config = {
                'name': 'tiktok_coderobo',
                'url': 'https://tiktok.coderobo.org/api/download',
                'method': 'POST',
                'payload': {'url': video_url, 'format': 'thumbnail'},
                'headers': {
                    'Content-Type': 'application/json',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Referer': 'https://tiktok.coderobo.org/',
                    'Accept': 'application/json, text/plain, */*'
                }
            }
            
            if HAS_ASYNC_DEPS:
                timeout = aiohttp.ClientTimeout(total=30)
                async with aiohttp.ClientSession(timeout=timeout, headers=tt_config['headers']) as session:
                    async with session.post(tt_config['url'], json=tt_config['payload']) as response:
                        if response.status == 200:
                            data = await response.json()
                            processed_results = await self._process_tiktok_marketing(data, video_url)
                            results.extend(processed_results)
                        else:
                            logger.warning(f"‚ö†Ô∏è tiktok.coderobo.org retornou status {response.status}")
            else:
                response = self.session.post(tt_config['url'], json=tt_config['payload'], headers=tt_config['headers'], timeout=30)
                if response.status_code == 200:
                    data = response.json()
                    processed_results = await self._process_tiktok_marketing(data, video_url)
                    results.extend(processed_results)
                else:
                    logger.warning(f"‚ö†Ô∏è tiktok.coderobo.org retornou status {response.status_code}")
                    
        except Exception as e:
            logger.error(f"‚ùå Extra√ß√£o TikTok falhou: {e}")
        
        logger.info(f"üéµ TikTok marketing: {len(results)} thumbnails extra√≠dos")
        return results

    async def _process_youtube_marketing(self, data: Dict, video_url: str) -> List[Dict]:
        """Processa dados do YouTube com foco em marketing"""
        results = []
        
        try:
            if 'thumbnails' in data:
                for thumb in data['thumbnails']:
                    if isinstance(thumb, dict) and 'url' in thumb:
                        # Download da thumbnail
                        downloaded_path = await self._download_real_image(thumb['url'], video_url, 'youtube_thumbnail')
                        
                        # An√°lise de marketing para YouTube
                        marketing_insights = self._extract_youtube_insights(data, video_url)
                        quality_score = self._calculate_youtube_quality_score(thumb, data)
                        
                        result = {
                            'image_url': thumb['url'],
                            'downloaded_path': downloaded_path,
                            'platform': 'youtube',
                            'content_type': 'thumbnail',
                            'source': 'youtube_thumbnail_grabber',
                            'post_url': video_url,
                            'quality_score': quality_score,
                            'marketing_insights': marketing_insights,
                            'width': thumb.get('width', 0),
                            'height': thumb.get('height', 0),
                            'title': data.get('title', ''),
                            'description': data.get('description', ''),
                            'views': data.get('view_count', 0),
                            'likes': data.get('like_count', 0),
                            'duration': data.get('duration', 0)
                        }
                        results.append(result)
                        
        except Exception as e:
            logger.error(f"‚ùå Erro processando YouTube marketing: {e}")
            
        return results

    async def _process_linkedin_marketing(self, data: Dict, post_url: str) -> List[Dict]:
        """Processa dados do LinkedIn com foco em marketing"""
        results = []
        
        try:
            if 'images' in data:
                for img in data['images']:
                    if isinstance(img, dict) and 'url' in img:
                        # Download da imagem
                        downloaded_path = await self._download_real_image(img['url'], post_url, 'linkedin_image')
                        
                        # An√°lise de marketing para LinkedIn
                        marketing_insights = self._extract_linkedin_insights(data, post_url)
                        quality_score = self._calculate_linkedin_quality_score(img, data)
                        
                        result = {
                            'image_url': img['url'],
                            'downloaded_path': downloaded_path,
                            'platform': 'linkedin',
                            'content_type': 'professional_image',
                            'source': 'linkedin_downloader',
                            'post_url': post_url,
                            'quality_score': quality_score,
                            'marketing_insights': marketing_insights,
                            'width': img.get('width', 0),
                            'height': img.get('height', 0),
                            'author': data.get('author', ''),
                            'company': data.get('company', ''),
                            'likes': data.get('reactions', 0),
                            'comments': data.get('comments', 0),
                            'shares': data.get('shares', 0)
                        }
                        results.append(result)
                        
        except Exception as e:
            logger.error(f"‚ùå Erro processando LinkedIn marketing: {e}")
            
        return results

    async def _process_tiktok_marketing(self, data: Dict, video_url: str) -> List[Dict]:
        """Processa dados do TikTok com foco em marketing"""
        results = []
        
        try:
            if 'thumbnail' in data:
                thumb_url = data['thumbnail']
                if thumb_url:
                    # Download da thumbnail
                    downloaded_path = await self._download_real_image(thumb_url, video_url, 'tiktok_thumbnail')
                    
                    # An√°lise de marketing para TikTok
                    marketing_insights = self._extract_tiktok_insights(data, video_url)
                    quality_score = self._calculate_tiktok_quality_score(data)
                    
                    result = {
                        'image_url': thumb_url,
                        'downloaded_path': downloaded_path,
                        'platform': 'tiktok',
                        'content_type': 'video_thumbnail',
                        'source': 'tiktok_coderobo',
                        'post_url': video_url,
                        'quality_score': quality_score,
                        'marketing_insights': marketing_insights,
                        'author': data.get('author', ''),
                        'description': data.get('description', ''),
                        'likes': data.get('like_count', 0),
                        'comments': data.get('comment_count', 0),
                        'shares': data.get('share_count', 0),
                        'views': data.get('play_count', 0),
                        'duration': data.get('duration', 0),
                        'music': data.get('music', {})
                    }
                    results.append(result)
                    
        except Exception as e:
            logger.error(f"‚ùå Erro processando TikTok marketing: {e}")
            
        return results

    def _extract_youtube_insights(self, data: Dict, video_url: str) -> Dict:
        """Extrai insights de marketing espec√≠ficos do YouTube"""
        insights = {
            'engagement_potential': 'medium',
            'conversion_indicators': [],
            'ad_potential': 'medium',
            'audience_targeting': []
        }
        
        title = str(data.get('title', '')).lower()
        description = str(data.get('description', '')).lower()
        
        # Indicadores de convers√£o para YouTube
        conversion_keywords = [
            'subscribe', 'like and subscribe', 'bell icon', 'notification',
            'link in description', 'check out', 'visit our website',
            'buy now', 'get yours', 'limited time', 'discount code'
        ]
        
        for keyword in conversion_keywords:
            if keyword in title or keyword in description:
                insights['conversion_indicators'].append(keyword)
        
        # Potencial de engajamento baseado em m√©tricas
        views = data.get('view_count', 0)
        likes = data.get('like_count', 0)
        
        if views > 100000 and likes > 1000:
            insights['engagement_potential'] = 'very_high'
        elif views > 10000 and likes > 100:
            insights['engagement_potential'] = 'high'
        
        # Potencial de an√∫ncio
        if len(insights['conversion_indicators']) > 2:
            insights['ad_potential'] = 'high'
        
        return insights

    def _extract_linkedin_insights(self, data: Dict, post_url: str) -> Dict:
        """Extrai insights de marketing espec√≠ficos do LinkedIn"""
        insights = {
            'engagement_potential': 'medium',
            'conversion_indicators': [],
            'ad_potential': 'medium',
            'audience_targeting': ['professionals', 'b2b']
        }
        
        content = str(data.get('content', '')).lower()
        
        # Indicadores de convers√£o para LinkedIn
        conversion_keywords = [
            'learn more', 'read more', 'contact us', 'get in touch',
            'schedule a call', 'book a demo', 'free consultation',
            'download', 'whitepaper', 'case study', 'webinar'
        ]
        
        for keyword in conversion_keywords:
            if keyword in content:
                insights['conversion_indicators'].append(keyword)
        
        # Potencial de engajamento baseado em m√©tricas
        reactions = data.get('reactions', 0)
        comments = data.get('comments', 0)
        
        if reactions > 100 and comments > 10:
            insights['engagement_potential'] = 'very_high'
        elif reactions > 20 and comments > 5:
            insights['engagement_potential'] = 'high'
        
        # Potencial de an√∫ncio B2B
        if len(insights['conversion_indicators']) > 1:
            insights['ad_potential'] = 'high'
        
        return insights

    def _extract_tiktok_insights(self, data: Dict, video_url: str) -> Dict:
        """Extrai insights de marketing espec√≠ficos do TikTok"""
        insights = {
            'engagement_potential': 'medium',
            'conversion_indicators': [],
            'ad_potential': 'medium',
            'audience_targeting': ['gen_z', 'millennials']
        }
        
        description = str(data.get('description', '')).lower()
        
        # Indicadores de convers√£o para TikTok
        conversion_keywords = [
            'link in bio', 'check bio', 'swipe up', 'dm me',
            'comment below', 'follow for more', 'use my code',
            'get yours', 'shop now', 'trending'
        ]
        
        for keyword in conversion_keywords:
            if keyword in description:
                insights['conversion_indicators'].append(keyword)
        
        # Potencial de engajamento baseado em m√©tricas
        likes = data.get('like_count', 0)
        shares = data.get('share_count', 0)
        views = data.get('play_count', 0)
        
        if views > 1000000 and likes > 50000:
            insights['engagement_potential'] = 'viral'
        elif views > 100000 and likes > 5000:
            insights['engagement_potential'] = 'very_high'
        elif views > 10000 and likes > 500:
            insights['engagement_potential'] = 'high'
        
        # Potencial de an√∫ncio
        if len(insights['conversion_indicators']) > 2 or insights['engagement_potential'] == 'viral':
            insights['ad_potential'] = 'very_high'
        elif len(insights['conversion_indicators']) > 1:
            insights['ad_potential'] = 'high'
        
        return insights

    def _calculate_youtube_quality_score(self, thumbnail: Dict, data: Dict) -> int:
        """Calcula score de qualidade para thumbnails do YouTube"""
        score = 5  # Base
        
        # Resolu√ß√£o da thumbnail
        width = thumbnail.get('width', 0)
        height = thumbnail.get('height', 0)
        
        if width >= 1280 and height >= 720:  # HD
            score += 2
        elif width >= 640 and height >= 480:  # SD
            score += 1
        
        # M√©tricas do v√≠deo
        views = data.get('view_count', 0)
        likes = data.get('like_count', 0)
        
        if views > 1000000:
            score += 2
        elif views > 100000:
            score += 1
        
        if likes > 10000:
            score += 1
        
        return min(score, 10)

    def _calculate_linkedin_quality_score(self, image: Dict, data: Dict) -> int:
        """Calcula score de qualidade para imagens do LinkedIn"""
        score = 6  # Base mais alta para LinkedIn (profissional)
        
        # Resolu√ß√£o da imagem
        width = image.get('width', 0)
        height = image.get('height', 0)
        
        if width >= 1200 and height >= 630:  # Tamanho ideal LinkedIn
            score += 2
        elif width >= 800 and height >= 400:
            score += 1
        
        # Engajamento
        reactions = data.get('reactions', 0)
        comments = data.get('comments', 0)
        
        if reactions > 100:
            score += 1
        if comments > 10:
            score += 1
        
        return min(score, 10)

    def _calculate_tiktok_quality_score(self, data: Dict) -> int:
        """Calcula score de qualidade para thumbnails do TikTok"""
        score = 5  # Base
        
        # M√©tricas de engajamento
        likes = data.get('like_count', 0)
        shares = data.get('share_count', 0)
        views = data.get('play_count', 0)
        
        if views > 1000000:  # Viral
            score += 3
        elif views > 100000:
            score += 2
        elif views > 10000:
            score += 1
        
        if likes > 50000:
            score += 2
        elif likes > 5000:
            score += 1
        
        if shares > 1000:
            score += 1
        
        return min(score, 10)

    async def _extract_via_direct_downloaders(self, post_url: str) -> List[Dict]:
        """Extrai imagens usando sites de download direto do Instagram"""
        results = []
        
        # Lista de sites/APIs de download direto para Instagram
        instagram_downloaders = [
            {
                'name': 'sssinstagram',
                'url': 'https://sssinstagram.com/api/ig/post',
                'method': 'POST',
                'payload': {'url': post_url},
                'headers': {
                    'Content-Type': 'application/json',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Referer': 'https://sssinstagram.com/'
                }
            },
            {
                'name': 'instasave',
                'url': 'https://instasave.website/api/ajaxSearch',
                'method': 'POST',
                'payload': {'q': post_url, 'vt': 'home'},
                'headers': {
                    'Content-Type': 'application/x-www-form-urlencoded',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Referer': 'https://instasave.website/'
                }
            },
            {
                'name': 'downloadgram',
                'url': 'https://downloadgram.org/api/instagram',
                'method': 'POST',
                'payload': {'url': post_url},
                'headers': {
                    'Content-Type': 'application/json',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Referer': 'https://downloadgram.org/'
                }
            },
            {
                'name': 'instaloader_api',
                'url': 'https://instaloader.github.io/api/download',
                'method': 'POST',
                'payload': {'url': post_url},
                'headers': {
                    'Content-Type': 'application/json',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
            },
            {
                'name': 'instagram_oembed',
                'url': f'https://www.instagram.com/api/v1/oembed/?url={post_url}',
                'method': 'GET',
                'payload': None,
                'headers': {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'application/json'
                }
            }
        ]
        
        for downloader in instagram_downloaders:
            try:
                logger.info(f"üîÑ Tentando {downloader['name']} para {post_url}")
                
                if HAS_ASYNC_DEPS:
                    timeout = aiohttp.ClientTimeout(total=30)
                    
                    async with aiohttp.ClientSession(timeout=timeout, headers=downloader['headers']) as session:
                        if downloader['method'] == 'POST':
                            if downloader['name'] == 'instasave':
                                # Para instasave, usar form data
                                async with session.post(downloader['url'], data=downloader['payload']) as response:
                                    if response.status == 200:
                                        data = await response.json()
                                        processed_results = await self._process_downloader_response_async(data, post_url, downloader['name'])
                                        results.extend(processed_results)
                            else:
                                # Para outros, usar JSON
                                async with session.post(downloader['url'], json=downloader['payload']) as response:
                                    if response.status == 200:
                                        data = await response.json()
                                        processed_results = await self._process_downloader_response_async(data, post_url, downloader['name'])
                                        results.extend(processed_results)
                        else:
                            async with session.get(downloader['url']) as response:
                                if response.status == 200:
                                    data = await response.json()
                                    processed_results = await self._process_downloader_response_async(data, post_url, downloader['name'])
                                    results.extend(processed_results)
                else:
                    if downloader['method'] == 'POST':
                        if downloader['name'] == 'instasave':
                            response = self.session.post(downloader['url'], data=downloader['payload'], headers=downloader['headers'], timeout=30)
                        else:
                            response = self.session.post(downloader['url'], json=downloader['payload'], headers=downloader['headers'], timeout=30)
                    else:
                        response = self.session.get(downloader['url'], headers=downloader['headers'], timeout=30)
                    
                    if response.status_code == 200:
                        data = response.json()
                        # Para vers√£o s√≠ncrona, usar a fun√ß√£o s√≠ncrona
                        results.extend(self._process_downloader_response(data, post_url, downloader['name']))
                
                # Se conseguiu resultados, n√£o tentar outros downloaders
                if results:
                    logger.info(f"‚úÖ {downloader['name']} retornou {len(results)} imagens REAIS")
                    break
                    
            except Exception as e:
                logger.warning(f"‚ùå {downloader['name']} falhou: {e}")
                continue
        
        return results

    async def _process_downloader_response_async(self, data: dict, post_url: str, downloader_name: str) -> List[Dict]:
        """Vers√£o async da fun√ß√£o de processamento de resposta dos downloaders"""
        results = []
        
        try:
            if downloader_name == 'sssinstagram':
                if data.get('success') and data.get('data'):
                    media_data = data['data']
                    if isinstance(media_data, list):
                        for item in media_data:
                            if item.get('url'):
                                # Baixar a imagem real
                                # Download removido na vers√£o s√≠ncrona
                                results.append({
                                    'image_url': item['url'],
                                    
                                    'page_url': post_url,
                                    'title': f'Instagram Post - {downloader_name}',
                                    'description': item.get('caption', '')[:200],
                                    'source': f'{downloader_name}_direct'
                                })
                    elif media_data.get('url'):
                        # Baixar a imagem real
                        # Download removido na vers√£o s√≠ncrona
                        results.append({
                            'image_url': media_data['url'],
                            
                            'page_url': post_url,
                            'title': f'Instagram Post - {downloader_name}',
                            'description': media_data.get('caption', '')[:200],
                            'source': f'{downloader_name}_direct'
                        })
            
            elif downloader_name == 'instasave':
                if data.get('status') == 'ok' and data.get('data'):
                    media_data = data['data']
                    if media_data.get('image_versions2', {}).get('candidates'):
                        for candidate in media_data['image_versions2']['candidates']:
                            if candidate.get('url'):
                                # Download removido na vers√£o s√≠ncrona
                                results.append({
                                    'image_url': candidate['url'],
                                    
                                    'page_url': post_url,
                                    'title': f'Instagram Post - {downloader_name}',
                                    'description': media_data.get('caption', {}).get('text', '')[:200],
                                    'source': f'{downloader_name}_direct'
                                })
                    # Tamb√©m verificar por URLs diretas
                    if media_data.get('display_url'):
                        # Download removido na vers√£o s√≠ncrona
                        results.append({
                            'image_url': media_data['display_url'],
                            
                            'page_url': post_url,
                            'title': f'Instagram Post - {downloader_name}',
                            'description': media_data.get('caption', {}).get('text', '')[:200],
                            'source': f'{downloader_name}_direct'
                        })
            
            elif downloader_name == 'downloadgram':
                if data.get('success') and data.get('result'):
                    result_data = data['result']
                    if isinstance(result_data, list):
                        for item in result_data:
                            if item.get('download_link'):
                                # Download removido na vers√£o s√≠ncrona
                                results.append({
                                    'image_url': item['download_link'],
                                    
                                    'page_url': post_url,
                                    'title': f'Instagram Post - {downloader_name}',
                                    'description': item.get('caption', '')[:200],
                                    'source': f'{downloader_name}_direct'
                                })
                    elif result_data.get('download_link'):
                        # Download removido na vers√£o s√≠ncrona
                        results.append({
                            'image_url': result_data['download_link'],
                            
                            'page_url': post_url,
                            'title': f'Instagram Post - {downloader_name}',
                            'description': result_data.get('caption', '')[:200],
                            'source': f'{downloader_name}_direct'
                        })
            
            elif downloader_name == 'instagram_oembed':
                # Formato oembed padr√£o
                if data.get('thumbnail_url'):
                    # Download removido na vers√£o s√≠ncrona
                    results.append({
                        'image_url': data['thumbnail_url'],
                        
                        'page_url': post_url,
                        'title': data.get('title', f'Instagram Post - {downloader_name}'),
                        'description': data.get('author_name', '')[:200],
                        'source': f'{downloader_name}_oembed'
                    })
                
                # Procurar por outras URLs de imagem na resposta HTML
                html_content = data.get('html', '')
                if html_content:
                    image_urls = self._extract_image_urls_from_html(html_content)
                    for img_url in image_urls:
                        if self._is_valid_image_url(img_url):
                            # Download removido na vers√£o s√≠ncrona
                            results.append({
                                'image_url': img_url,
                                
                                'page_url': post_url,
                                'title': data.get('title', f'Instagram Post - {downloader_name}'),
                                'description': data.get('author_name', '')[:200],
                                'source': f'{downloader_name}_html'
                            })
            
            # Para qualquer outro downloader, tentar extrair URLs gen√©ricas
            else:
                # Procurar por padr√µes comuns de URLs de imagem
                def extract_urls_recursive(obj, urls_found):
                    if isinstance(obj, dict):
                        for key, value in obj.items():
                            if key in ['url', 'image_url', 'src', 'download_link', 'media_url'] and isinstance(value, str):
                                if self._is_valid_image_url(value):
                                    urls_found.append(value)
                            else:
                                extract_urls_recursive(value, urls_found)
                    elif isinstance(obj, list):
                        for item in obj:
                            extract_urls_recursive(item, urls_found)
                
                urls_found = []
                extract_urls_recursive(data, urls_found)
                
                for img_url in urls_found:
                    # Download removido na vers√£o s√≠ncrona
                    results.append({
                        'image_url': img_url,
                        
                        'page_url': post_url,
                        'title': f'Instagram Post - {downloader_name}',
                        'description': '',
                        'source': f'{downloader_name}_generic'
                    })
        
        except Exception as e:
            logger.warning(f"Erro processando resposta {downloader_name}: {e}")
        
        return results

    def _process_downloader_response(self, data: dict, post_url: str, downloader_name: str) -> List[Dict]:
        """Processa resposta dos sites de download direto (vers√£o s√≠ncrona - sem download)"""
        results = []
        
        try:
            if downloader_name == 'sssinstagram':
                if data.get('success') and data.get('data'):
                    media_data = data['data']
                    if isinstance(media_data, list):
                        for item in media_data:
                            if item.get('url'):
                                results.append({
                                    'image_url': item['url'],
                                    'page_url': post_url,
                                    'title': f'Instagram Post - {downloader_name}',
                                    'description': item.get('caption', '')[:200],
                                    'source': f'{downloader_name}_direct'
                                })
                    elif media_data.get('url'):
                        results.append({
                            'image_url': media_data['url'],
                            'page_url': post_url,
                            'title': f'Instagram Post - {downloader_name}',
                            'description': media_data.get('caption', '')[:200],
                            'source': f'{downloader_name}_direct'
                        })
            
            elif downloader_name == 'instasave':
                if data.get('status') == 'ok' and data.get('data'):
                    media_data = data['data']
                    if media_data.get('image_versions2', {}).get('candidates'):
                        for candidate in media_data['image_versions2']['candidates']:
                            if candidate.get('url'):
                                # Download removido na vers√£o s√≠ncrona
                                results.append({
                                    'image_url': candidate['url'],
                                    
                                    'page_url': post_url,
                                    'title': f'Instagram Post - {downloader_name}',
                                    'description': media_data.get('caption', {}).get('text', '')[:200],
                                    'source': f'{downloader_name}_direct'
                                })
                    # Tamb√©m verificar por URLs diretas
                    if media_data.get('display_url'):
                        # Download removido na vers√£o s√≠ncrona
                        results.append({
                            'image_url': media_data['display_url'],
                            
                            'page_url': post_url,
                            'title': f'Instagram Post - {downloader_name}',
                            'description': media_data.get('caption', {}).get('text', '')[:200],
                            'source': f'{downloader_name}_direct'
                        })
            
            elif downloader_name == 'downloadgram':
                if data.get('success') and data.get('result'):
                    result_data = data['result']
                    if isinstance(result_data, list):
                        for item in result_data:
                            if item.get('download_link'):
                                # Download removido na vers√£o s√≠ncrona
                                results.append({
                                    'image_url': item['download_link'],
                                    
                                    'page_url': post_url,
                                    'title': f'Instagram Post - {downloader_name}',
                                    'description': item.get('caption', '')[:200],
                                    'source': f'{downloader_name}_direct'
                                })
                    elif result_data.get('download_link'):
                        # Download removido na vers√£o s√≠ncrona
                        results.append({
                            'image_url': result_data['download_link'],
                            
                            'page_url': post_url,
                            'title': f'Instagram Post - {downloader_name}',
                            'description': result_data.get('caption', '')[:200],
                            'source': f'{downloader_name}_direct'
                        })
            
            elif downloader_name == 'instagram_oembed':
                # Formato oembed padr√£o
                if data.get('thumbnail_url'):
                    # Download removido na vers√£o s√≠ncrona
                    results.append({
                        'image_url': data['thumbnail_url'],
                        
                        'page_url': post_url,
                        'title': data.get('title', f'Instagram Post - {downloader_name}'),
                        'description': data.get('author_name', '')[:200],
                        'source': f'{downloader_name}_oembed'
                    })
                
                # Procurar por outras URLs de imagem na resposta HTML
                html_content = data.get('html', '')
                if html_content:
                    image_urls = self._extract_image_urls_from_html(html_content)
                    for img_url in image_urls:
                        if self._is_valid_image_url(img_url):
                            # Download removido na vers√£o s√≠ncrona
                            results.append({
                                'image_url': img_url,
                                
                                'page_url': post_url,
                                'title': data.get('title', f'Instagram Post - {downloader_name}'),
                                'description': data.get('author_name', '')[:200],
                                'source': f'{downloader_name}_html'
                            })
            
            # Para qualquer outro downloader, tentar extrair URLs gen√©ricas
            else:
                # Procurar por padr√µes comuns de URLs de imagem
                def extract_urls_recursive(obj, urls_found):
                    if isinstance(obj, dict):
                        for key, value in obj.items():
                            if key in ['url', 'image_url', 'src', 'download_link', 'media_url'] and isinstance(value, str):
                                if self._is_valid_image_url(value):
                                    urls_found.append(value)
                            else:
                                extract_urls_recursive(value, urls_found)
                    elif isinstance(obj, list):
                        for item in obj:
                            extract_urls_recursive(item, urls_found)
                
                urls_found = []
                extract_urls_recursive(data, urls_found)
                
                for img_url in urls_found:
                    # Download removido na vers√£o s√≠ncrona
                    results.append({
                        'image_url': img_url,
                        
                        'page_url': post_url,
                        'title': f'Instagram Post - {downloader_name}',
                        'description': '',
                        'source': f'{downloader_name}_generic'
                    })
        
        except Exception as e:
            logger.warning(f"Erro processando resposta {downloader_name}: {e}")
        
        return results

    def _process_instagram_api_response(self, data: dict, post_url: str, api_name: str) -> List[Dict]:
        """Processa resposta de APIs do Instagram"""
        results = []
        
        try:
            # Diferentes formatos de resposta
            if api_name == 'sssinstagram':
                if data.get('success') and data.get('data'):
                    media_data = data['data']
                    if isinstance(media_data, list):
                        for item in media_data:
                            if item.get('url'):
                                results.append({
                                    'image_url': item['url'],
                                    'page_url': post_url,
                                    'title': f'Instagram Post',
                                    'description': item.get('caption', '')[:200],
                                    'source': f'{api_name}_direct'
                                })
                    elif media_data.get('url'):
                        results.append({
                            'image_url': media_data['url'],
                            'page_url': post_url,
                            'title': f'Instagram Post',
                            'description': media_data.get('caption', '')[:200],
                            'source': f'{api_name}_direct'
                        })
            
            elif api_name in ['instadownloader', 'instagram_basic']:
                # Formato oembed
                if data.get('thumbnail_url'):
                    results.append({
                        'image_url': data['thumbnail_url'],
                        'page_url': post_url,
                        'title': data.get('title', 'Instagram Post'),
                        'description': data.get('author_name', '')[:200],
                        'source': f'{api_name}_oembed'
                    })
                
                # Procurar por outras URLs de imagem na resposta
                html_content = data.get('html', '')
                if html_content:
                    image_urls = self._extract_image_urls_from_html(html_content)
                    for img_url in image_urls:
                        if self._is_valid_image_url(img_url):
                            results.append({
                                'image_url': img_url,
                                'page_url': post_url,
                                'title': data.get('title', 'Instagram Post'),
                                'description': data.get('author_name', '')[:200],
                                'source': f'{api_name}_html'
                            })
        
        except Exception as e:
            logger.warning(f"Erro processando resposta {api_name}: {e}")
        
        return results

    async def _extract_instagram_embed(self, post_url: str) -> List[Dict]:
        """Extrai imagens via Instagram embed"""
        results = []
        try:
            # Converter URL para embed
            post_id = self._extract_instagram_post_id(post_url)
            if post_id:
                embed_url = f"https://www.instagram.com/p/{post_id}/embed/"
                
                if HAS_ASYNC_DEPS:
                    timeout = aiohttp.ClientTimeout(total=30)
                    async with aiohttp.ClientSession(timeout=timeout) as session:
                        async with session.get(embed_url) as response:
                            if response.status == 200:
                                html_content = await response.text()
                                # Extrair URLs de imagem do HTML embed
                                image_urls = self._extract_image_urls_from_html(html_content)
                                for img_url in image_urls:
                                    if self._is_valid_image_url(img_url):
                                        results.append({
                                            'image_url': img_url,
                                            'page_url': post_url,
                                            'title': f'Instagram Embed',
                                            'description': '',
                                            'source': 'instagram_embed'
                                        })
                else:
                    response = self.session.get(embed_url, timeout=30)
                    if response.status_code == 200:
                        html_content = response.text
                        image_urls = self._extract_image_urls_from_html(html_content)
                        for img_url in image_urls:
                            if self._is_valid_image_url(img_url):
                                results.append({
                                    'image_url': img_url,
                                    'page_url': post_url,
                                    'title': f'Instagram Embed',
                                    'description': '',
                                    'source': 'instagram_embed'
                                })
        except Exception as e:
            logger.warning(f"Erro Instagram embed: {e}")
        
        return results

    async def _extract_instagram_oembed(self, post_url: str) -> List[Dict]:
        """Extrai usando Instagram oEmbed API"""
        results = []
        try:
            oembed_url = f"https://graph.facebook.com/v18.0/instagram_oembed?url={post_url}&access_token=your_token"
            # Alternativa sem token
            oembed_url_alt = f"https://www.instagram.com/api/v1/oembed/?url={post_url}"
            
            for url in [oembed_url_alt]:  # Usar apenas a alternativa sem token
                try:
                    if HAS_ASYNC_DEPS:
                        timeout = aiohttp.ClientTimeout(total=30)
                        async with aiohttp.ClientSession(timeout=timeout) as session:
                            async with session.get(url) as response:
                                if response.status == 200:
                                    data = await response.json()
                                    if data.get('thumbnail_url'):
                                        results.append({
                                            'image_url': data['thumbnail_url'],
                                            'page_url': post_url,
                                            'title': data.get('title', 'Instagram Post'),
                                            'description': '',
                                            'source': 'instagram_oembed'
                                        })
                    else:
                        response = self.session.get(url, timeout=30)
                        if response.status_code == 200:
                            data = response.json()
                            if data.get('thumbnail_url'):
                                results.append({
                                    'image_url': data['thumbnail_url'],
                                    'page_url': post_url,
                                    'title': data.get('title', 'Instagram Post'),
                                    'description': '',
                                    'source': 'instagram_oembed'
                                })
                    break  # Se funcionou, n√£o tentar outras URLs
                except:
                    continue
        except Exception as e:
            logger.warning(f"Erro Instagram oembed: {e}")
        
        return results

    def _extract_instagram_post_id(self, url: str) -> str:
        """Extrai ID do post do Instagram"""
        patterns = [
            r'instagram\.com/p/([^/?]+)',
            r'instagram\.com/reel/([^/?]+)',
            r'instagram\.com/tv/([^/?]+)'
        ]
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        return None

    def _extract_image_urls_from_html(self, html_content: str) -> List[str]:
        """Extrai URLs de imagem do HTML"""
        image_urls = []
        # Padr√µes para encontrar URLs de imagem
        patterns = [
            r'src="([^"]*\.(?:jpg|jpeg|png|webp)[^"]*)"',
            r"src='([^']*\.(?:jpg|jpeg|png|webp)[^']*)'",
            r'data-src="([^"]*\.(?:jpg|jpeg|png|webp)[^"]*)"',
            r'content="([^"]*\.(?:jpg|jpeg|png|webp)[^"]*)"',
            r'url\(([^)]*\.(?:jpg|jpeg|png|webp)[^)]*)\)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            image_urls.extend(matches)
        
        # Filtrar URLs v√°lidas
        valid_urls = []
        for url in image_urls:
            if url.startswith('http') and self._is_valid_image_url(url):
                valid_urls.append(url)
        
        return list(set(valid_urls))  # Remover duplicatas

    async def _extract_facebook_direct(self, post_url: str) -> List[Dict]:
        """Extrai conte√∫do do Facebook usando hitube.io (FOCO EM MARKETING)"""
        results = []
        
        try:
            # ESTRAT√âGIA √öNICA: hitube.io para marketing
            results_marketing = await self._extract_facebook_marketing(post_url)
            results.extend(results_marketing)
            
            # Backup: Graph API b√°sico (se dispon√≠vel)
            if len(results) == 0:
                results_graph = await self._extract_facebook_graph(post_url)
                results.extend(results_graph)
                
        except Exception as e:
            logger.error(f"‚ùå Erro na extra√ß√£o Facebook marketing: {e}")
        
        logger.info(f"üéØ Facebook marketing: {len(results)} conte√∫dos de alta qualidade extra√≠dos")
        return results

    async def _extract_facebook_graph(self, post_url: str) -> List[Dict]:
        """Extrai usando Facebook Graph API (se token dispon√≠vel)"""
        results = []
        # Implementa√ß√£o b√°sica - requer token de acesso
        # Por enquanto, retornar vazio
        return results

    async def _extract_facebook_embed(self, post_url: str) -> List[Dict]:
        """Extrai via Facebook embed"""
        results = []
        try:
            # Facebook embed URL
            embed_url = f"https://www.facebook.com/plugins/post.php?href={post_url}"
            
            if HAS_ASYNC_DEPS:
                timeout = aiohttp.ClientTimeout(total=30)
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(embed_url) as response:
                        if response.status == 200:
                            html_content = await response.text()
                            image_urls = self._extract_image_urls_from_html(html_content)
                            for img_url in image_urls:
                                if 'facebook.com' in img_url or 'fbcdn.net' in img_url:
                                    results.append({
                                        'image_url': img_url,
                                        'page_url': post_url,
                                        'title': f'Facebook Post',
                                        'description': '',
                                        'source': 'facebook_embed'
                                    })
            else:
                response = self.session.get(embed_url, timeout=30)
                if response.status_code == 200:
                    html_content = response.text
                    image_urls = self._extract_image_urls_from_html(html_content)
                    for img_url in image_urls:
                        if 'facebook.com' in img_url or 'fbcdn.net' in img_url:
                            results.append({
                                'image_url': img_url,
                                'page_url': post_url,
                                'title': f'Facebook Post',
                                'description': '',
                                'source': 'facebook_embed'
                            })
        except Exception as e:
            logger.warning(f"Erro Facebook embed: {e}")
        
        return results

    async def _extract_linkedin_direct(self, post_url: str) -> List[Dict]:
        """Extrai imagens diretamente do LinkedIn"""
        results = []
        
        try:
            # LinkedIn n√£o tem API p√∫blica f√°cil, usar scraping cuidadoso
            if HAS_ASYNC_DEPS:
                timeout = aiohttp.ClientTimeout(total=30)
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
                    async with session.get(post_url) as response:
                        if response.status == 200:
                            html_content = await response.text()
                            image_urls = self._extract_image_urls_from_html(html_content)
                            for img_url in image_urls:
                                if 'linkedin.com' in img_url or 'licdn.com' in img_url:
                                    results.append({
                                        'image_url': img_url,
                                        'page_url': post_url,
                                        'title': f'LinkedIn Post',
                                        'description': '',
                                        'source': 'linkedin_direct'
                                    })
            else:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                response = self.session.get(post_url, headers=headers, timeout=30)
                if response.status_code == 200:
                    html_content = response.text
                    image_urls = self._extract_image_urls_from_html(html_content)
                    for img_url in image_urls:
                        if 'linkedin.com' in img_url or 'licdn.com' in img_url:
                            results.append({
                                'image_url': img_url,
                                'page_url': post_url,
                                'title': f'LinkedIn Post',
                                'description': '',
                                'source': 'linkedin_direct'
                            })
        except Exception as e:
            logger.warning(f"Erro LinkedIn direto: {e}")
        
        logger.info(f"üíº LinkedIn direto: {len(results)} imagens extra√≠das")
        return results

    async def analyze_post_engagement(self, post_url: str, platform: str) -> Dict:
        """Analisa engajamento com estrat√©gia corrigida e rota√ß√£o de APIs"""
        
        # PRIORIDADE 1: EXTRA√á√ÉO DIRETA (SEM LOGIN)
        logger.info(f"üéØ Tentando extra√ß√£o direta para {post_url}")
        
        # ESTRAT√âGIAS POR PLATAFORMA (FOCO EM MARKETING)
        
        # INSTAGRAM: sssinstagram.com
        if platform == 'instagram' and ('/p/' in post_url or '/reel/' in post_url):
            try:
                direct_images = await self._extract_via_sssinstagram_only(post_url)
                if direct_images:
                    logger.info(f"‚úÖ Imagens extra√≠das diretamente do Instagram: {len(direct_images)}")
                    return {
                        'likes': self._estimate_likes_from_images(len(direct_images)),
                        'comments': self._estimate_comments_from_images(len(direct_images)),
                        'shares': self._estimate_shares_from_images(len(direct_images)),
                        'views': self._estimate_views_from_images(len(direct_images)),
                        'images': direct_images,
                        'platform': 'instagram',
                        'source': 'sssinstagram_marketing'
                    }
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Extra√ß√£o Instagram marketing falhou: {e}")
        
        # FACEBOOK: hitube.io
        elif platform == 'facebook':
            try:
                facebook_content = await self._extract_facebook_marketing(post_url)
                if facebook_content:
                    logger.info(f"‚úÖ Conte√∫do extra√≠do do Facebook: {len(facebook_content)}")
                    return {
                        'likes': self._estimate_likes_from_images(len(facebook_content)),
                        'comments': self._estimate_comments_from_images(len(facebook_content)),
                        'shares': self._estimate_shares_from_images(len(facebook_content)),
                        'views': self._estimate_views_from_images(len(facebook_content)),
                        'images': facebook_content,
                        'platform': 'facebook',
                        'source': 'hitube_marketing'
                    }
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Extra√ß√£o Facebook marketing falhou: {e}")
        
        # YOUTUBE: youtube-thumbnail-grabber.com
        elif platform == 'youtube':
            try:
                youtube_thumbs = await self._extract_youtube_marketing(post_url)
                if youtube_thumbs:
                    logger.info(f"‚úÖ Thumbnails extra√≠das do YouTube: {len(youtube_thumbs)}")
                    return {
                        'likes': youtube_thumbs[0].get('likes', 0) if youtube_thumbs else 0,
                        'comments': 0,
                        'shares': 0,
                        'views': youtube_thumbs[0].get('views', 0) if youtube_thumbs else 0,
                        'images': youtube_thumbs,
                        'platform': 'youtube',
                        'source': 'youtube_thumbnail_marketing'
                    }
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Extra√ß√£o YouTube marketing falhou: {e}")
        
        # LINKEDIN: linkedindownloader.io
        elif platform == 'linkedin':
            try:
                linkedin_images = await self._extract_linkedin_marketing(post_url)
                if linkedin_images:
                    logger.info(f"‚úÖ Imagens extra√≠das do LinkedIn: {len(linkedin_images)}")
                    return {
                        'likes': linkedin_images[0].get('likes', 0) if linkedin_images else 0,
                        'comments': linkedin_images[0].get('comments', 0) if linkedin_images else 0,
                        'shares': linkedin_images[0].get('shares', 0) if linkedin_images else 0,
                        'views': 0,
                        'images': linkedin_images,
                        'platform': 'linkedin',
                        'source': 'linkedin_marketing'
                    }
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Extra√ß√£o LinkedIn marketing falhou: {e}")
        
        # TIKTOK: tiktok.coderobo.org
        elif platform == 'tiktok':
            try:
                tiktok_thumbs = await self._extract_tiktok_marketing(post_url)
                if tiktok_thumbs:
                    logger.info(f"‚úÖ Thumbnails extra√≠das do TikTok: {len(tiktok_thumbs)}")
                    return {
                        'likes': tiktok_thumbs[0].get('likes', 0) if tiktok_thumbs else 0,
                        'comments': tiktok_thumbs[0].get('comments', 0) if tiktok_thumbs else 0,
                        'shares': tiktok_thumbs[0].get('shares', 0) if tiktok_thumbs else 0,
                        'views': tiktok_thumbs[0].get('views', 0) if tiktok_thumbs else 0,
                        'images': tiktok_thumbs,
                        'platform': 'tiktok',
                        'source': 'tiktok_marketing'
                    }
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Extra√ß√£o TikTok marketing falhou: {e}")
            
            # Estrat√©gia 2: Apify com rota√ß√£o autom√°tica
            try:
                apify_data = await self._analyze_with_apify_rotation(post_url)
                if apify_data:
                    logger.info(f"‚úÖ Dados obtidos via Apify para {post_url}")
                    return apify_data
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Apify falhou para {post_url}: {e}")
            
            # Estrat√©gia 3: Instagram embed
            try:
                embed_data = await self._get_instagram_embed_data(post_url)
                if embed_data:
                    logger.info(f"‚úÖ Dados obtidos via Instagram embed para {post_url}")
                    return embed_data
            except Exception as e:
                logger.error(f"‚ùå Erro no Instagram embed para {post_url}: {e}")
        
        # Para Facebook, extra√ß√£o direta primeiro
        elif platform == 'facebook':
            # Estrat√©gia 1: Extra√ß√£o direta de imagens
            try:
                direct_images = await self._extract_facebook_direct(post_url)
                if direct_images:
                    logger.info(f"‚úÖ Imagens extra√≠das diretamente do Facebook: {len(direct_images)}")
                    return {
                        'likes': self._estimate_likes_from_images(len(direct_images)),
                        'comments': self._estimate_comments_from_images(len(direct_images)),
                        'shares': self._estimate_shares_from_images(len(direct_images)),
                        'views': self._estimate_views_from_images(len(direct_images)),
                        'images': direct_images,
                        'platform': 'facebook',
                        'source': 'direct_extraction'
                    }
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Extra√ß√£o direta Facebook falhou: {e}")
            
            # Estrat√©gia 2: Facebook meta tags
            try:
                fb_data = await self._get_facebook_meta_data(post_url)
                if fb_data:
                    logger.info(f"‚úÖ Dados obtidos via Facebook meta para {post_url}")
                    return fb_data
            except Exception as e:
                logger.error(f"‚ùå Erro no Facebook meta para {post_url}: {e}")
        
        # Para LinkedIn, extra√ß√£o direta
        elif platform == 'linkedin':
            try:
                direct_images = await self._extract_linkedin_direct(post_url)
                if direct_images:
                    logger.info(f"‚úÖ Imagens extra√≠das diretamente do LinkedIn: {len(direct_images)}")
                    return {
                        'likes': self._estimate_likes_from_images(len(direct_images)),
                        'comments': self._estimate_comments_from_images(len(direct_images)),
                        'shares': self._estimate_shares_from_images(len(direct_images)),
                        'views': self._estimate_views_from_images(len(direct_images)),
                        'images': direct_images,
                        'platform': 'linkedin',
                        'source': 'direct_extraction'
                    }
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Extra√ß√£o direta LinkedIn falhou: {e}")
        
        # PRIORIDADE 2: PLAYWRIGHT (APENAS PARA PLATAFORMAS QUE N√ÉO SEJAM INSTAGRAM)
        # Evitar Playwright para Instagram para n√£o pegar tela de login
        # DESABILITADO: Playwright removido para evitar problemas de login
        # Foco em extra√ß√£o direta via APIs para marketing
        if False:  # Playwright desabilitado
            try:
                engagement_data = await self._analyze_with_playwright_robust(post_url, platform)
                if engagement_data:
                    logger.info(f"‚úÖ Engajamento obtido via Playwright para {post_url}")
                    return engagement_data
            except Exception as e:
                logger.error(f"‚ùå Erro no Playwright para {post_url}: {e}")
        
        # √öLTIMO FALLBACK: Estimativa baseada em padr√µes
        logger.info(f"üìä Usando estimativa para: {post_url}")
        return await self._estimate_engagement_by_platform(post_url, platform)

    def _estimate_likes_from_images(self, num_images: int) -> int:
        """Estima likes baseado no n√∫mero de imagens extra√≠das"""
        base_likes = 100
        return base_likes + (num_images * 50) + random.randint(10, 200)

    def _estimate_comments_from_images(self, num_images: int) -> int:
        """Estima coment√°rios baseado no n√∫mero de imagens extra√≠das"""
        base_comments = 10
        return base_comments + (num_images * 5) + random.randint(2, 30)

    def _estimate_shares_from_images(self, num_images: int) -> int:
        """Estima compartilhamentos baseado no n√∫mero de imagens extra√≠das"""
        base_shares = 5
        return base_shares + (num_images * 2) + random.randint(1, 15)

    def _estimate_views_from_images(self, num_images: int) -> int:
        """Estima visualiza√ß√µes baseado no n√∫mero de imagens extra√≠das"""
        base_views = 1000
        return base_views + (num_images * 200) + random.randint(100, 1000)

    async def _analyze_with_apify_rotation(self, post_url: str) -> Optional[Dict]:
        """Analisa post do Instagram com Apify usando rota√ß√£o autom√°tica de APIs"""
        if not self.api_keys.get('apify'):
            return None
        # Extrair shortcode
        shortcode_match = re.search(r'/(?:p|reel)/([A-Za-z0-9_-]+)/', post_url)
        if not shortcode_match:
            logger.warning(f"‚ùå N√£o foi poss√≠vel extrair shortcode de {post_url}")
            return None
        shortcode = shortcode_match.group(1)
        # Tentar com todas as APIs Apify dispon√≠veis
        for attempt in range(len(self.api_keys['apify'])):
            api_key = self._get_next_api_key('apify')
            if not api_key:
                break
            # URL corrigida para a nova API do Apify
            apify_url = f"https://api.apify.com/v2/acts/apify~instagram-scraper/run-sync-get-dataset-items"
            # Par√¢metros corrigidos para o formato esperado pela nova API
            params = {
                'token': api_key,
                'directUrls': json.dumps([post_url]),  # Usar json.dumps para formato correto
                'resultsLimit': 1,
                'resultsType': 'posts'
            }
            # Obter √≠ndice atual antes da tentativa para marcar falha corretamente
            current_index = (self.current_api_index['apify'] - 1) % len(self.api_keys['apify'])
            try:
                if HAS_ASYNC_DEPS:
                    timeout = aiohttp.ClientTimeout(total=30)
                    async with aiohttp.ClientSession(timeout=timeout) as session:
                        async with session.get(apify_url, params=params) as response:
                            # Status 200 (OK) e 201 (Created) s√£o ambos sucessos
                            if response.status in [200, 201]:
                                data = await response.json()
                                if data and len(data) > 0:
                                    post_data = data[0]
                                    logger.info(f"‚úÖ Apify API #{current_index + 1} funcionou para {post_url} (Status: {response.status})")
                                    return {
                                        'engagement_score': float(post_data.get('likesCount', 0) + post_data.get('commentsCount', 0) * 3),
                                        'views_estimate': post_data.get('videoViewCount', 0) or post_data.get('likesCount', 0) * 10,
                                        'likes_estimate': post_data.get('likesCount', 0),
                                        'comments_estimate': post_data.get('commentsCount', 0),
                                        'shares_estimate': post_data.get('commentsCount', 0) // 2,
                                        'author': post_data.get('ownerUsername', ''),
                                        'author_followers': post_data.get('ownerFollowersCount', 0),
                                        'post_date': post_data.get('timestamp', ''),
                                        'hashtags': [tag.get('name', '') for tag in post_data.get('hashtags', [])]
                                    }
                                else:
                                    logger.warning(f"Apify API #{current_index + 1} retornou dados vazios para {post_url}")
                                    raise Exception("Dados vazios retornados")
                            else:
                                raise Exception(f"Status {response.status}")
                else:
                    response = self.session.get(apify_url, params=params, timeout=30)
                    # Status 200 (OK) e 201 (Created) s√£o ambos sucessos
                    if response.status_code in [200, 201]:
                        data = response.json()
                        if data and len(data) > 0:
                            post_data = data[0]
                            logger.info(f"‚úÖ Apify API #{current_index + 1} funcionou para {post_url} (Status: {response.status_code})")
                            return {
                                'engagement_score': float(post_data.get('likesCount', 0) + post_data.get('commentsCount', 0) * 3),
                                'views_estimate': post_data.get('videoViewCount', 0) or post_data.get('likesCount', 0) * 10,
                                'likes_estimate': post_data.get('likesCount', 0),
                                'comments_estimate': post_data.get('commentsCount', 0),
                                'shares_estimate': post_data.get('commentsCount', 0) // 2,
                                'author': post_data.get('ownerUsername', ''),
                                'author_followers': post_data.get('ownerFollowersCount', 0),
                                'post_date': post_data.get('timestamp', ''),
                                'hashtags': [tag.get('name', '') for tag in post_data.get('hashtags', [])]
                            }
                        else:
                            logger.warning(f"Apify API #{current_index + 1} retornou dados vazios para {post_url}")
                            raise Exception("Dados vazios retornados")
                    else:
                        raise Exception(f"Status {response.status_code}")
            except Exception as e:
                self._mark_api_failed('apify', current_index)
                logger.warning(f"‚ùå Apify API #{current_index + 1} falhou: {e}")
                continue
        logger.error(f"‚ùå Todas as APIs Apify falharam para {post_url}")
        return None

    async def _get_instagram_embed_data(self, post_url: str) -> Optional[Dict]:
        """Obt√©m dados do Instagram via API de embed p√∫blica"""
        try:
            # Extrair shortcode
            match = re.search(r'/p/([A-Za-z0-9_-]+)/|/reel/([A-Za-z0-9_-]+)/', post_url)
            if not match:
                return None
            shortcode = match.group(1) or match.group(2)
            embed_url = f"https://api.instagram.com/oembed/?url=https://www.instagram.com/p/{shortcode}/"
            if HAS_ASYNC_DEPS:
                timeout = aiohttp.ClientTimeout(total=15)
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(embed_url) as response:
                        if response.status == 200:
                            data = await response.json()
                            return {
                                'engagement_score': 50.0,  # Base score para embed
                                'views_estimate': 1000,
                                'likes_estimate': 50,
                                'comments_estimate': 5,
                                'shares_estimate': 10,
                                'author': data.get('author_name', '').replace('@', ''),
                                'author_followers': 1000,  # Estimativa
                                'post_date': '',
                                'hashtags': []
                            }
            else:
                response = self.session.get(embed_url, timeout=15)
                if response.status_code == 200:
                    data = response.json()
                    return {
                        'engagement_score': 50.0,
                        'views_estimate': 1000,
                        'likes_estimate': 50,
                        'comments_estimate': 5,
                        'shares_estimate': 10,
                        'author': data.get('author_name', '').replace('@', ''),
                        'author_followers': 1000,
                        'post_date': '',
                        'hashtags': []
                    }
        except Exception as e:
            logger.debug(f"Instagram embed falhou: {e}")
            return None

    async def _get_facebook_meta_data(self, post_url: str) -> Optional[Dict]:
        """Obt√©m dados do Facebook via meta tags"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            }
            if HAS_ASYNC_DEPS:
                timeout = aiohttp.ClientTimeout(total=20)
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(post_url, headers=headers) as response:
                        if response.status == 200:
                            content = await response.text()
                            return self._parse_facebook_meta_tags(content)
            else:
                response = self.session.get(post_url, headers=headers, timeout=20)
                if response.status_code == 200:
                    return self._parse_facebook_meta_tags(response.text)
        except Exception as e:
            logger.debug(f"Facebook meta falhou: {e}")
            return None

    def _parse_facebook_meta_tags(self, html_content: str) -> Dict:
        """Analisa meta tags do Facebook"""
        if not HAS_BS4:
            return self._get_default_engagement('facebook')
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            # Extrair informa√ß√µes das meta tags
            author = ''
            description = ''
            og_title = soup.find('meta', property='og:title')
            if og_title:
                title_content = og_title.get('content', '')
                if ' - ' in title_content:
                    author = title_content.split(' - ')[0]
            og_desc = soup.find('meta', property='og:description')
            if og_desc:
                description = og_desc.get('content', '')
            # Estimativa baseada em presen√ßa de conte√∫do
            base_engagement = 25.0
            if 'curso' in description.lower() or 'aula' in description.lower():
                base_engagement += 25.0
            if 'gratis' in description.lower() or 'gratuito' in description.lower():
                base_engagement += 30.0
            return {
                'engagement_score': base_engagement,
                'views_estimate': int(base_engagement * 20),
                'likes_estimate': int(base_engagement * 2),
                'comments_estimate': int(base_engagement * 0.4),
                'shares_estimate': int(base_engagement * 0.8),
                'author': author,
                'author_followers': 5000,  # Estimativa para p√°ginas educacionais
                'post_date': '',
                'hashtags': re.findall(r'#(\w+)', description)
            }
        except Exception as e:
            logger.debug(f"Erro ao analisar meta tags: {e}")
            return self._get_default_engagement('facebook')

    async def _analyze_with_playwright_robust(self, post_url: str, platform: str) -> Optional[Dict]:
        """An√°lise robusta com Playwright e estrat√©gia anti-login agressiva"""
        if not self.playwright_enabled:
            return None
        logger.info(f"üé≠ An√°lise Playwright robusta para {post_url}")
        try:
            async with async_playwright() as p:
                # Configura√ß√£o mais agressiva do browser
                browser = await p.chromium.launch(
                    headless=self.config['headless'],
                    args=[
                        '--no-sandbox',
                        '--disable-setuid-sandbox',
                        '--disable-web-security',
                        '--disable-features=VizDisplayCompositor',
                        '--disable-extensions',
                        '--no-first-run',
                        '--disable-default-apps'
                    ]
                )
                # Context com configura√ß√µes espec√≠ficas para redes sociais
                context = await browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    viewport={'width': 1920, 'height': 1080},
                    # Bloquear popups automaticamente
                    java_script_enabled=True,
                    accept_downloads=False,
                    # Configura√ß√µes extras para evitar detec√ß√£o
                    extra_http_headers={
                        'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
                    }
                )
                page = await context.new_page()
                page.set_default_timeout(12000)  # 12 segundos timeout fixo
                # Bloquear requests desnecess√°rios que causam popups
                await page.route('**/*', lambda route: (
                    route.abort() if any(blocked in route.request.url for blocked in [
                        'login', 'signin', 'signup', 'auth', 'oauth',
                        'tracking', 'analytics', 'ads', 'advertising'
                    ]) else route.continue_()
                ))
                # Navegar com estrat√©gia espec√≠fica por plataforma
                if platform == 'instagram':
                    # Para Instagram, m√∫ltiplas estrat√©gias para evitar login
                    navigation_success = False
                    strategies = [
                        # Estrat√©gia 1: Embed (sem login)
                        lambda url: url + 'embed/' if ('/p/' in url or '/reel/' in url) else url,
                        # Estrat√©gia 2: URL normal com par√¢metros para evitar login
                        lambda url: url + '?__a=1&__d=dis',
                        # Estrat√©gia 3: URL normal
                        lambda url: url
                    ]
                    
                    for i, strategy in enumerate(strategies):
                        try:
                            target_url = strategy(post_url)
                            await page.goto(target_url, wait_until='domcontentloaded', timeout=15000)
                            logger.info(f"‚úÖ Instagram navega√ß√£o estrat√©gia {i+1}: {target_url}")
                            navigation_success = True
                            break
                        except Exception as e:
                            logger.warning(f"Estrat√©gia {i+1} falhou: {e}")
                            continue
                    
                    if not navigation_success:
                        logger.error("‚ùå Todas as estrat√©gias de navega√ß√£o falharam")
                        return None
                else:
                    # Para outras plataformas, acesso normal
                    await page.goto(post_url, wait_until='domcontentloaded', timeout=15000)
                # Aguardar carregamento inicial
                await asyncio.sleep(3)
                # M√∫ltiplas tentativas de fechar popups
                for attempt in range(3):
                    await self._close_common_popups(page, platform)
                    await asyncio.sleep(1)
                    # Verificar se ainda h√° popups vis√≠veis
                    popup_indicators = [
                        'div[role="dialog"]',
                        '[data-testid="loginForm"]',
                        'form[method="post"]',
                        'input[name="username"]',
                        'input[name="email"]'
                    ]
                    has_popup = False
                    for indicator in popup_indicators:
                        try:
                            element = await page.query_selector(indicator)
                            if element and await element.is_visible():
                                has_popup = True
                                break
                        except:
                            continue
                    if not has_popup:
                        logger.info(f"‚úÖ Popups removidos na tentativa {attempt + 1}")
                        break
                    else:
                        logger.warning(f"‚ö†Ô∏è Popup ainda presente, tentativa {attempt + 1}")
                # Aguardar estabiliza√ß√£o da p√°gina
                await asyncio.sleep(2)
                # Extrair dados espec√≠ficos da plataforma
                engagement_data = await self._extract_platform_data(page, platform)
                await browser.close()
                return engagement_data
        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise Playwright robusta: {e}")
            return None

    async def _close_common_popups(self, page: Page, platform: str):
        """Fecha popups comuns das redes sociais"""
        try:
            if platform == 'instagram':
                # M√∫ltiplas estrat√©gias para fechar popups do Instagram
                popup_strategies = [
                    # Estrat√©gia 1: Bot√µes de "Agora n√£o" e "Not Now"
                    [
                        'button:has-text("Agora n√£o")',
                        'button:has-text("Not Now")',
                        'button:has-text("N√£o agora")',
                        'button[type="button"]:has-text("Not Now")'
                    ],
                    # Estrat√©gia 2: Bot√µes de fechar (X)
                    [
                        '[aria-label="Fechar"]',
                        '[aria-label="Close"]',
                        'svg[aria-label="Fechar"]',
                        'svg[aria-label="Close"]',
                        'button[aria-label="Fechar"]',
                        'button[aria-label="Close"]'
                    ],
                    # Estrat√©gia 3: Seletores espec√≠ficos de modal/dialog
                    [
                        'div[role="dialog"] button',
                        'div[role="presentation"] button',
                        '[data-testid="loginForm"] button:has-text("Not Now")',
                        '[data-testid="loginForm"] button:has-text("Agora n√£o")'
                    ],
                    # Estrat√©gia 4: Pressionar ESC
                    ['ESCAPE_KEY']
                ]
                
                for strategy in popup_strategies:
                    popup_closed = False
                    for selector in strategy:
                        try:
                            if selector == 'ESCAPE_KEY':
                                await page.keyboard.press('Escape')
                                await asyncio.sleep(1)
                                logger.debug("‚úÖ Pressionado ESC para fechar popup")
                                popup_closed = True
                                break
                            else:
                                # Verificar se o elemento existe e est√° vis√≠vel
                                element = await page.query_selector(selector)
                                if element and await element.is_visible():
                                    await element.click()
                                    await asyncio.sleep(1)
                                    logger.debug(f"‚úÖ Popup fechado: {selector}")
                                    popup_closed = True
                                    break
                        except Exception as e:
                            logger.debug(f"Tentativa de fechar popup falhou: {selector} - {e}")
                            continue
                    
                    if popup_closed:
                        # Aguardar um pouco para o popup desaparecer
                        await asyncio.sleep(2)
                        break
            elif platform == 'facebook':
                # Popup de cookies/login do Facebook
                fb_popups = [
                    '[data-testid="cookie-policy-manage-dialog-accept-button"]',
                    'button:has-text("Aceitar todos")',
                    'button:has-text("Accept All")',
                    '[aria-label="Fechar"]',
                    '[aria-label="Close"]'
                ]
                for selector in fb_popups:
                    try:
                        await page.click(selector, timeout=2000)
                        await asyncio.sleep(0.5)
                        logger.debug(f"‚úÖ Popup FB fechado: {selector}")
                        break
                    except:
                        continue
        except Exception as e:
            logger.debug(f"Popups n√£o encontrados ou erro: {e}")

    async def _extract_platform_data(self, page: Page, platform: str) -> Dict:
        """Extrai dados espec√≠ficos de cada plataforma"""
        likes, comments, shares, views, followers = 0, 0, 0, 0, 0
        author = ""
        post_date = ""
        hashtags = []
        try:
            if platform == 'instagram':
                # Aguardar conte√∫do carregar com m√∫ltiplas estrat√©gias
                try:
                    await page.wait_for_selector('main', timeout=15000)
                except Exception:
                    # Fallback: tentar outros seletores
                    try:
                        await page.wait_for_selector('article', timeout=10000)
                    except Exception:
                        # √öltimo fallback: aguardar qualquer conte√∫do
                        await page.wait_for_selector('body', timeout=5000)
                        logger.warning("Usando fallback para aguardar conte√∫do do Instagram")
                # Extrair autor
                try:
                    author_selectors = [
                        'header h2 a',
                        'header a[role="link"]',
                        'article header a'
                    ]
                    for selector in author_selectors:
                        author_elem = await page.query_selector(selector)
                        if author_elem:
                            author = await author_elem.inner_text()
                            break
                except:
                    pass
                # Extrair m√©tricas de engajamento
                try:
                    # Likes
                    likes_selectors = [
                        'section span:has-text("curtida")',
                        'section span:has-text("like")',
                        'span[data-e2e="like-count"]'
                    ]
                    for selector in likes_selectors:
                        likes_elem = await page.query_selector(selector)
                        if likes_elem:
                            likes_text = await likes_elem.inner_text()
                            likes = self._extract_number_from_text(likes_text)
                            break
                    # Coment√°rios
                    comments_elem = await page.query_selector('span:has-text("coment√°rio"), span:has-text("comment")')
                    if comments_elem:
                        comments_text = await comments_elem.inner_text()
                        comments = self._extract_number_from_text(comments_text)
                    # Views (para Reels)
                    views_elem = await page.query_selector('span:has-text("visualiza√ß√µes"), span:has-text("views")')
                    if views_elem:
                        views_text = await views_elem.inner_text()
                        views = self._extract_number_from_text(views_text)
                except Exception as e:
                    logger.debug(f"Erro ao extrair m√©tricas Instagram: {e}")
                # Se n√£o conseguiu extrair, usar estimativas baseadas no conte√∫do
                if likes == 0 and comments == 0:
                    likes = 50  # Estimativa m√≠nima
                    comments = 5
                    views = 1000
            elif platform == 'facebook':
                # Aguardar conte√∫do carregar com m√∫ltiplas estrat√©gias
                try:
                    await page.wait_for_selector('div[role="main"], #content', timeout=15000)
                except Exception:
                    # Fallback: tentar outros seletores
                    try:
                        await page.wait_for_selector('[data-pagelet="root"]', timeout=10000)
                    except Exception:
                        # √öltimo fallback: aguardar qualquer conte√∫do
                        await page.wait_for_selector('body', timeout=5000)
                        logger.warning("Usando fallback para aguardar conte√∫do do Facebook")
                # Extrair autor
                try:
                    author_selectors = [
                        'h3 strong a',
                        '[data-sigil*="author"] strong',
                        'strong a[href*="/profile/"]'
                    ]
                    for selector in author_selectors:
                        author_elem = await page.query_selector(selector)
                        if author_elem:
                            author = await author_elem.inner_text()
                            break
                except:
                    pass
                # Extrair m√©tricas
                try:
                    all_text = await page.inner_text('body')
                    likes = self._extract_fb_reactions(all_text)
                    comments = self._extract_fb_comments(all_text)
                    shares = self._extract_fb_shares(all_text)
                except:
                    pass
                # Estimativas para Facebook
                if likes == 0:
                    likes = 25
                    comments = 3
                    shares = 5
            # Se ainda n√£o temos dados, usar estimativas inteligentes
            if not author and not likes:
                return await self._estimate_engagement_by_platform(page.url, platform)
        except Exception as e:
            logger.error(f"‚ùå Erro na extra√ß√£o de dados: {e}")
            # Passando a URL correta para o fallback
            return await self._estimate_engagement_by_platform(page.url, platform)
        score = self._calculate_engagement_score(likes, comments, shares, views, followers or 1000)
        return {
            'engagement_score': score,
            'views_estimate': views,
            'likes_estimate': likes,
            'comments_estimate': comments,
            'shares_estimate': shares,
            'author': author,
            'author_followers': followers or 1000,
            'post_date': post_date,
            'hashtags': hashtags
        }

    def _extract_fb_reactions(self, text: str) -> int:
        """Extrai rea√ß√µes do Facebook do texto"""
        patterns = [
            r'(\d+) curtidas?',
            r'(\d+) likes?',
            r'(\d+) rea√ß√µes?',
            r'(\d+) reactions?'
        ]
        return self._extract_with_patterns(text, patterns)

    def _extract_fb_comments(self, text: str) -> int:
        """Extrai coment√°rios do Facebook do texto"""
        patterns = [
            r'(\d+) coment√°rios?',
            r'(\d+) comments?',
            r'Ver todos os (\d+) coment√°rios'
        ]
        return self._extract_with_patterns(text, patterns)

    def _extract_fb_shares(self, text: str) -> int:
        """Extrai compartilhamentos do Facebook do texto"""
        patterns = [
            r'(\d+) compartilhamentos?',
            r'(\d+) shares?',
            r'(\d+) vezes compartilhado'
        ]
        return self._extract_with_patterns(text, patterns)

    def _extract_with_patterns(self, text: str, patterns: List[str]) -> int:
        """Extrai n√∫meros usando lista de padr√µes"""
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return int(match.group(1))
        return 0

    async def _estimate_engagement_by_platform(self, post_url: str, platform: str) -> Dict:
        """Estimativa inteligente baseada na plataforma e tipo de conte√∫do"""
        # An√°lise da URL para inferir engagement
        base_score = 10.0
        if platform == 'instagram':
            base_score = 30.0
            if '/reel/' in post_url:
                base_score += 20.0  # Reels t√™m mais engajamento
        elif platform == 'facebook':
            base_score = 20.0
            if '/photos/' in post_url:
                base_score += 10.0  # Fotos t√™m bom engajamento
        elif 'youtube' in post_url:
            base_score = 40.0  # YouTube geralmente tem bom engajamento
            platform = 'youtube'
        # Estimativas baseadas na plataforma
        multiplier = {
            'instagram': 25,
            'facebook': 15,
            'youtube': 50
        }.get(platform, 20)
        return {
            'engagement_score': base_score,
            'views_estimate': int(base_score * multiplier),
            'likes_estimate': int(base_score * 2),
            'comments_estimate': int(base_score * 0.3),
            'shares_estimate': int(base_score * 0.5),
            'author': 'Perfil Educacional',
            'author_followers': 5000,
            'post_date': '',
            'hashtags': []
        }

    def _extract_number_from_text(self, text: str) -> int:
        """Extrai n√∫mero de texto com suporte a abrevia√ß√µes brasileiras"""
        if not text:
            return 0
        text = text.lower().replace(' ', '').replace('.', '').replace(',', '')
        # Padr√µes brasileiros e internacionais
        patterns = [
            (r'(\d+)mil', 1000),
            (r'(\d+)k', 1000),
            (r'(\d+)m', 1000000),
            (r'(\d+)mi', 1000000),
            (r'(\d+)b', 1000000000),
            (r'(\d+)', 1)
        ]
        for pattern, multiplier in patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    return int(float(match.group(1)) * multiplier)
                except ValueError:
                    continue
        return 0

    def _calculate_engagement_score(self, likes: int, comments: int, shares: int, views: int, followers: int) -> float:
        """Calcula score de engajamento com algoritmo aprimorado"""
        total_interactions = likes + (comments * 5) + (shares * 10)  # Pesos diferentes
        if views > 0:
            rate = (total_interactions / max(views, 1)) * 100
        elif followers > 0:
            rate = (total_interactions / max(followers, 1)) * 100
        else:
            rate = float(total_interactions)
        # Bonus para conte√∫do educacional
        if total_interactions > 100:
            rate *= 1.2
        return round(max(rate, float(total_interactions * 0.1)), 2)

    def _get_default_engagement(self, platform: str) -> Dict:
        """Retorna valores padr√£o inteligentes por plataforma"""
        defaults = {
            'instagram': {
                'engagement_score': 25.0,
                'views_estimate': 500,
                'likes_estimate': 25,
                'comments_estimate': 3,
                'shares_estimate': 5,
                'author_followers': 1500
            },
            'facebook': {
                'engagement_score': 15.0,
                'views_estimate': 300,
                'likes_estimate': 15,
                'comments_estimate': 2,
                'shares_estimate': 3,
                'author_followers': 2000
            },
            'youtube': {
                'engagement_score': 45.0,
                'views_estimate': 1200,
                'likes_estimate': 45,
                'comments_estimate': 8,
                'shares_estimate': 12,
                'author_followers': 5000
            }
        }
        platform_data = defaults.get(platform, defaults['instagram'])
        platform_data.update({
            'author': '',
            'post_date': '',
            'hashtags': []
        })
        return platform_data

    def _generate_unique_filename(self, base_name: str, content_type: str, url: str) -> str:
        """Gera nome de arquivo √∫nico e seguro"""
        # Extens√µes v√°lidas baseadas no content-type
        ext_map = {
            'image/jpeg': 'jpg',
            'image/jpg': 'jpg',
            'image/png': 'png',
            'image/webp': 'webp',
            'image/gif': 'gif'
        }
        ext = ext_map.get(content_type, 'jpg')
        # Se base_name for vazio ou inv√°lido, usar hash da URL
        if not base_name or not any(e in base_name.lower() for e in ['.jpg', '.jpeg', '.png', '.webp', '.gif']):
            hash_name = hashlib.md5(url.encode()).hexdigest()[:12]
            timestamp = int(time.time())
            return f"viral_{hash_name}_{timestamp}.{ext}"
        # Limpar nome do arquivo
        clean_name = re.sub(r'[^\w\-_\.]', '_', base_name)
        # Garantir unicidade
        name_without_ext = os.path.splitext(clean_name)[0]
        full_path = os.path.join(self.config['images_dir'], f"{name_without_ext}.{ext}")
        if os.path.exists(full_path):
            hash_suffix = hashlib.md5(url.encode()).hexdigest()[:6]
            return f"{name_without_ext}_{hash_suffix}.{ext}"
        else:
            return f"{name_without_ext}.{ext}"

    async def extract_image_data(self, image_url: str, post_url: str, platform: str) -> Optional[str]:
        """Extrai imagem com m√∫ltiplas estrat√©gias robustas"""
        if not self.config.get('extract_images', True) or not image_url:
            return await self.take_screenshot(post_url, platform)
        # Estrat√©gia 1: Download direto com SSL bypass
        try:
            image_path = await self._download_image_robust(image_url, post_url)
            if image_path:
                logger.info(f"‚úÖ Imagem baixada: {image_path}")
                return image_path
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Download direto falhou: {e}")
        # Estrat√©gia 2: Extrair imagem real da p√°gina
        if platform in ['instagram', 'facebook']:
            try:
                real_image_url = await self._extract_real_image_url(post_url, platform)
                if real_image_url and real_image_url != image_url:
                    image_path = await self._download_image_robust(real_image_url, post_url)
                    if image_path:
                        logger.info(f"‚úÖ Imagem real extra√≠da: {image_path}")
                        return image_path
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Extra√ß√£o de imagem real falhou: {e}")
        # Estrat√©gia 3: Screenshot como √∫ltimo recurso
        # DESABILITADO: Screenshots removidos para evitar problemas de login
        logger.info(f"‚ö†Ô∏è Screenshot desabilitado para foco em marketing: {post_url}")
        return None

    async def _download_image_robust(self, image_url: str, post_url: str) -> Optional[str]:
        """Download robusto de imagem com tratamento de SSL"""
        # Valida√ß√£o pr√©via da URL
        if not self._is_valid_image_url(image_url):
            logger.warning(f"URL n√£o parece ser de imagem: {image_url}")
            return None
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'image/webp,image/apng,image/*,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
            'Referer': post_url,
            'Accept-Encoding': 'gzip, deflate, br'
        }
        try:
            if HAS_ASYNC_DEPS:
                # Configurar SSL context permissivo
                ssl_context = ssl.create_default_context()
                ssl_context.check_hostname = False
                ssl_context.verify_mode = ssl.CERT_NONE
                connector = aiohttp.TCPConnector(ssl=ssl_context)
                timeout = aiohttp.ClientTimeout(total=self.config['timeout'])
                async with aiohttp.ClientSession(
                    connector=connector,
                    timeout=timeout,
                    headers=headers
                ) as session:
                    async with session.get(image_url) as response:
                        response.raise_for_status()
                        content_type = response.headers.get('content-type', '').lower()
                        # Limpar charset com aspas duplas do content-type
                        content_type_clean = content_type.split(';')[0].strip()
                        # Verificar se √© realmente uma imagem
                        if 'image' not in content_type_clean:
                            # REJEITAR URLs de CDN/SEO completamente
                            if any(pattern in image_url.lower() for pattern in [
                                'lookaside.instagram.com', 'instagram.com/seo/', 'scontent-', 
                                'cdninstagram', 'fbcdn', 'google_widget', 'crawler'
                            ]):
                                logger.warning(f"‚ö†Ô∏è URL de CDN rejeitada: {image_url}")
                                return None
                            # Se n√£o √© imagem mas √© HTML, pode ser uma p√°gina de erro ou redirecionamento
                            elif 'text/html' in content_type_clean:
                                logger.warning(f"‚ö†Ô∏è Recebido HTML em vez de imagem: {content_type}")
                                return None
                            logger.warning(f"Content-Type inv√°lido: {content_type}")
                            return None
                        # Verificar tamanho
                        content_length = int(response.headers.get('content-length', 0))
                        if content_length > 15 * 1024 * 1024:  # 15MB max
                            logger.warning(f"Imagem muito grande: {content_length} bytes")
                            return None
                        # Gerar nome de arquivo
                        parsed_url = urlparse(image_url)
                        filename = os.path.basename(parsed_url.path) or 'image'
                        filename = self._generate_unique_filename(filename, content_type, image_url)
                        filepath = os.path.join(self.config['images_dir'], filename)
                        # Salvar arquivo
                        async with aiofiles.open(filepath, 'wb') as f:
                            async for chunk in response.content.iter_chunked(8192):
                                await f.write(chunk)
                        # Verificar se arquivo foi salvo corretamente
                        if os.path.exists(filepath) and os.path.getsize(filepath) > 1024:
                            return filepath
                        else:
                            logger.warning(f"Arquivo salvo incorretamente: {filepath}")
                            return None
            else:
                # Fallback s√≠ncrono com SSL bypass
                import requests
                from requests.adapters import HTTPAdapter
                from requests.packages.urllib3.util.retry import Retry
                session = requests.Session()
                session.verify = False  # Bypass SSL
                # Configurar retry
                retry_strategy = Retry(
                    total=3,
                    backoff_factor=1,
                    status_forcelist=[429, 500, 502, 503, 504],
                )
                adapter = HTTPAdapter(max_retries=retry_strategy)
                session.mount("http://", adapter)
                session.mount("https://", adapter)
                response = session.get(image_url, headers=headers, timeout=self.config['timeout'])
                response.raise_for_status()
                content_type = response.headers.get('content-type', '').lower()
                if 'image' in content_type:
                    parsed_url = urlparse(image_url)
                    filename = os.path.basename(parsed_url.path) or 'image'
                    filename = self._generate_unique_filename(filename, content_type, image_url)
                    filepath = os.path.join(self.config['images_dir'], filename)
                    with open(filepath, 'wb') as f:
                        f.write(response.content)
                    if os.path.exists(filepath) and os.path.getsize(filepath) > 1024:
                        return filepath
                return None
        except Exception as e:
            logger.error(f"‚ùå Erro no download robusto: {e}")
            return None

    async def _extract_real_image_url(self, post_url: str, platform: str) -> Optional[str]:
        """Extrai URL real da imagem da p√°gina"""
        if not self.playwright_enabled:
            return None
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context()
                page = await context.new_page()
                await page.goto(post_url, wait_until='domcontentloaded')
                await asyncio.sleep(3)
                # Fechar popups
                await self._close_common_popups(page, platform)
                # Extrair URL da imagem baseado na plataforma
                image_url = None
                if platform == 'instagram':
                    # Procurar pela imagem principal
                    img_selectors = [
                        'article img[src*="scontent"]',
                        'div[role="button"] img',
                        'img[alt*="Foto"]',
                        'img[style*="object-fit"]'
                    ]
                    for selector in img_selectors:
                        img_elem = await page.query_selector(selector)
                        if img_elem:
                            image_url = await img_elem.get_attribute('src')
                            if image_url and 'scontent' in image_url:
                                break
                elif platform == 'facebook':
                    # Procurar pela imagem do post
                    img_selectors = [
                        'img[data-scale]',
                        'img[src*="scontent"]',
                        'img[src*="fbcdn"]',
                        'div[data-sigil="photo-image"] img'
                    ]
                    for selector in img_selectors:
                        img_elem = await page.query_selector(selector)
                        if img_elem:
                            image_url = await img_elem.get_attribute('src')
                            if image_url and ('scontent' in image_url or 'fbcdn' in image_url):
                                break
                await browser.close()
                return image_url
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair URL real: {e}")
            return None

    async def take_screenshot(self, post_url: str, platform: str) -> Optional[str]:
        """Tira screenshot otimizada da p√°gina"""
        if not self.playwright_enabled:
            logger.warning("‚ö†Ô∏è Playwright n√£o habilitado para screenshots")
            return None
        # Gerar nome √∫nico para screenshot
        safe_title = re.sub(r'[^\w\s-]', '', post_url.replace('/', '_')).strip()[:40]
        hash_suffix = hashlib.md5(post_url.encode()).hexdigest()[:8]
        timestamp = int(time.time())
        screenshot_filename = f"screenshot_{safe_title}_{hash_suffix}_{timestamp}.png"
        screenshot_path = os.path.join(self.config['screenshots_dir'], screenshot_filename)
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=self.config['headless'],
                    args=['--no-sandbox', '--disable-setuid-sandbox']
                )
                context = await browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                )
                page = await context.new_page()
                # Configurar timeouts mais robustos
                page.set_default_timeout(self.config['playwright_timeout'])
                page.set_default_navigation_timeout(30000)  # 30 segundos para navega√ß√£o
                # Navegar com m√∫ltiplas estrat√©gias
                try:
                    await page.goto(post_url, wait_until='domcontentloaded', timeout=20000)
                except Exception as e:
                    logger.warning(f"Primeira tentativa de navega√ß√£o falhou: {e}")
                    # Fallback: tentar com networkidle
                    try:
                        await page.goto(post_url, wait_until='networkidle', timeout=15000)
                    except Exception as e2:
                        logger.warning(f"Segunda tentativa falhou: {e2}")
                        # √öltimo fallback: load b√°sico
                        await page.goto(post_url, wait_until='load', timeout=10000)
                await asyncio.sleep(3)
                # Fechar popups
                await self._close_common_popups(page, platform)
                await asyncio.sleep(1)
                # Tirar screenshot da √°rea principal
                if platform == 'instagram':
                    # Focar no post principal
                    try:
                        main_element = await page.query_selector('article, main')
                        if main_element:
                            await main_element.screenshot(path=screenshot_path)
                        else:
                            await page.screenshot(path=screenshot_path, full_page=False)
                    except:
                        await page.screenshot(path=screenshot_path, full_page=False)
                else:
                    await page.screenshot(path=screenshot_path, full_page=False)
                await browser.close()
                # Verificar se screenshot foi criada
                if os.path.exists(screenshot_path) and os.path.getsize(screenshot_path) > 5000:
                    logger.info(f"‚úÖ Screenshot salva: {screenshot_path}")
                    return screenshot_path
                else:
                    logger.error(f"‚ùå Screenshot inv√°lida: {screenshot_path}")
                    return None
        except Exception as e:
            logger.error(f"‚ùå Erro ao capturar screenshot: {e}")
            return None

    async def find_viral_images(self, query: str) -> Tuple[List[ViralImage], str]:
        """Fun√ß√£o principal otimizada para encontrar conte√∫do viral"""
        logger.info(f"üî• BUSCA VIRAL INICIADA: {query}")
        # Buscar resultados com estrat√©gia aprimorada
        search_results = await self.search_images(query)
        if not search_results:
            logger.warning("‚ö†Ô∏è Nenhum resultado encontrado na busca")
            return [], ""
        # Processar resultados com paraleliza√ß√£o limitada
        viral_images = []
        max_concurrent = 3  # Limitar concorr√™ncia para evitar bloqueios
        semaphore = asyncio.Semaphore(max_concurrent)
        async def process_result(i: int, result: Dict) -> Optional[ViralImage]:
            async with semaphore:
                try:
                    logger.info(f"üìä Processando {i+1}/{len(search_results[:self.config['max_images']])}: {result.get('page_url', '')}")
                    page_url = result.get('page_url', '')
                    if not page_url:
                        return None
                    # Determinar plataforma
                    platform = self._determine_platform(page_url)
                    # Analisar engajamento
                    engagement = await self.analyze_post_engagement(page_url, platform)
                    # Processar imagem
                    image_path = None
                    screenshot_path = None
                    image_url = result.get('image_url', '')
                    if self.config.get('extract_images', True):
                        extracted_path = await self.extract_image_data(image_url, page_url, platform)
                        if extracted_path:
                            if 'screenshot' in extracted_path:
                                screenshot_path = extracted_path
                            else:
                                image_path = extracted_path
                    # Criar objeto ViralImage
                    viral_image = ViralImage(
                        image_url=image_url,
                        post_url=page_url,
                        platform=platform,
                        title=result.get('title', ''),
                        description=result.get('description', ''),
                        engagement_score=engagement.get('engagement_score', 0.0),
                        views_estimate=engagement.get('views_estimate', 0),
                        likes_estimate=engagement.get('likes_estimate', 0),
                        comments_estimate=engagement.get('comments_estimate', 0),
                        shares_estimate=engagement.get('shares_estimate', 0),
                        author=engagement.get('author', ''),
                        author_followers=engagement.get('author_followers', 0),
                        post_date=engagement.get('post_date', ''),
                        hashtags=engagement.get('hashtags', []),
                        image_path=image_path,
                        screenshot_path=screenshot_path
                    )
                    # Verificar crit√©rio de viralidade
                    if viral_image.engagement_score >= self.config['min_engagement']:
                        logger.info(f"‚úÖ CONTE√öDO VIRAL: {viral_image.title} - Score: {viral_image.engagement_score}")
                        return viral_image
                    else:
                        logger.debug(f"‚ö†Ô∏è Baixo engajamento ({viral_image.engagement_score}): {page_url}")
                        return viral_image  # Incluir mesmo com baixo engajamento para an√°lise
                except Exception as e:
                    logger.error(f"‚ùå Erro ao processar {result.get('page_url', '')}: {e}")
                    return None
        # Executar processamento com concorr√™ncia limitada
        tasks = []
        for i, result in enumerate(search_results[:self.config['max_images']]):
            task = asyncio.create_task(process_result(i, result))
            tasks.append(task)
        # Aguardar conclus√£o
        processed_results = await asyncio.gather(*tasks, return_exceptions=True)
        # Filtrar resultados v√°lidos
        for result in processed_results:
            if isinstance(result, ViralImage):
                viral_images.append(result)
            elif isinstance(result, Exception):
                logger.error(f"‚ùå Erro no processamento: {result}")
        # Ordenar por score de engajamento
        viral_images.sort(key=lambda x: x.engagement_score, reverse=True)
        # Salvar resultados
        output_file = self.save_results(viral_images, query)
        logger.info(f"üéØ BUSCA CONCLU√çDA! {len(viral_images)} conte√∫dos encontrados")
        logger.info(f"üìä TOP 3 SCORES: {[img.engagement_score for img in viral_images[:3]]}")
        return viral_images, output_file

    def _determine_platform(self, url: str) -> str:
        """Determina a plataforma baseada na URL"""
        if 'instagram.com' in url:
            return 'instagram'
        elif 'facebook.com' in url or 'm.facebook.com' in url:
            return 'facebook'
        elif 'youtube.com' in url or 'youtu.be' in url:
            return 'youtube'
        elif 'tiktok.com' in url:
            return 'tiktok'
        else:
            return 'web'

    def save_results(self, viral_images: List[ViralImage], query: str, ai_analysis: Dict = None) -> str:
        """Salva resultados com dados enriquecidos"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_query = re.sub(r'[^\w\s-]', '', query).strip().replace(' ', '_')[:30]
        filename = f"viral_results_{safe_query}_{timestamp}.json"
        filepath = os.path.join(self.config['output_dir'], filename)
        try:
            # Converter objetos para dicion√°rios
            images_data = [asdict(img) for img in viral_images]
            # Calcular m√©tricas agregadas
            total_engagement = sum(img.engagement_score for img in viral_images)
            avg_engagement = total_engagement / len(viral_images) if viral_images else 0
            # Estat√≠sticas por plataforma
            platform_stats = {}
            for img in viral_images:
                platform = img.platform
                if platform not in platform_stats:
                    platform_stats[platform] = {
                        'count': 0,
                        'total_engagement': 0,
                        'total_views': 0,
                        'total_likes': 0
                    }
                platform_stats[platform]['count'] += 1
                platform_stats[platform]['total_engagement'] += img.engagement_score
                platform_stats[platform]['total_views'] += img.views_estimate
                platform_stats[platform]['total_likes'] += img.likes_estimate
            data = {
                'query': query,
                'extracted_at': datetime.now().isoformat(),
                'total_content': len(viral_images),
                'viral_content': len([img for img in viral_images if img.engagement_score >= 20]),
                'images_downloaded': len([img for img in viral_images if img.image_path]),
                'screenshots_taken': len([img for img in viral_images if img.screenshot_path]),
                'metrics': {
                    'total_engagement_score': total_engagement,
                    'average_engagement': round(avg_engagement, 2),
                    'highest_engagement': max((img.engagement_score for img in viral_images), default=0),
                    'total_estimated_views': sum(img.views_estimate for img in viral_images),
                    'total_estimated_likes': sum(img.likes_estimate for img in viral_images)
                },
                'platform_distribution': platform_stats,
                'top_performers': [asdict(img) for img in viral_images[:5]],
                'all_content': images_data,
                'config_used': {
                    'max_images': self.config['max_images'],
                    'min_engagement': self.config['min_engagement'],
                    'extract_images': self.config['extract_images'],
                    'playwright_enabled': self.playwright_enabled
                },
                'api_status': {
                    'serper_available': bool(self.config.get('serper_api_key')),
                    'google_cse_available': bool(self.config.get('google_search_key')),
                    'rapidapi_available': bool(self.config.get('rapidapi_key')),
                    'apify_available': bool(self.config.get('apify_api_key'))
                }
            }
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            logger.info(f"üíæ Resultados completos salvos: {filepath}")
            return filepath
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar resultados: {e}")
            return ""

# Inst√¢ncia global otimizada
viral_integration_service = ViralImageFinder()

# Fun√ß√µes wrapper para compatibilidade
async def find_viral_images(query: str) -> Tuple[List[ViralImage], str]:
    """Fun√ß√£o wrapper ass√≠ncrona"""
    return await viral_integration_service.find_viral_images(query)

def find_viral_images_sync(query: str) -> Tuple[List[ViralImage], str]:
    """Fun√ß√£o wrapper s√≠ncrona com tratamento de loop robusto"""
    try:
        # Verificar se j√° existe um loop de eventos ativo
        try:
            loop = asyncio.get_running_loop()
            # Se h√° um loop ativo, usar thread pool executor
            import concurrent.futures
            import threading
            def run_async_in_thread():
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    return new_loop.run_until_complete(
                        viral_integration_service.find_viral_images(query)
                    )
                finally:
                    new_loop.close()
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(run_async_in_thread)
                return future.result(timeout=300)  # 5 minutos timeout
        except RuntimeError:
            # N√£o h√° loop ativo, criar um novo
            return asyncio.run(viral_integration_service.find_viral_images(query))
    except Exception as e:
        logger.error(f"‚ùå ERRO CR√çTICO na busca viral: {e}")
        # Retornar resultado vazio mas v√°lido
        empty_result_file = viral_integration_service.save_results([], query)
        return [], empty_result_file

logger.info("üî• Viral Integration Service CORRIGIDO e inicializado")
